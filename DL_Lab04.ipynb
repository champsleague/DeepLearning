{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORTcPmMc6lBq8TUa13JWlZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/champsleague/DeepLearning/blob/main/DL_Lab04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHtppKuu3LvQ",
        "outputId": "b30dd545-2d8b-4fc2-cd36-bc20f73a821d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 2])\n",
            "torch.Size([8, 1])\n",
            "\n",
            "————Before train————\n",
            "W: tensor([[0.],\n",
            "        [0.]], requires_grad=True)\n",
            "b: tensor([0.], requires_grad=True)\n",
            "\n",
            "\n",
            "epoch:0 cost:0.693147\n",
            "epoch:1 cost:0.672145\n",
            "epoch:2 cost:0.657294\n",
            "epoch:3 cost:0.646757\n",
            "epoch:4 cost:0.639247\n",
            "epoch:5 cost:0.633865\n",
            "epoch:6 cost:0.629987\n",
            "epoch:7 cost:0.627179\n",
            "epoch:8 cost:0.625136\n",
            "epoch:9 cost:0.623641\n",
            "epoch:10 cost:0.622544\n",
            "epoch:11 cost:0.621735\n",
            "epoch:12 cost:0.621136\n",
            "epoch:13 cost:0.620691\n",
            "epoch:14 cost:0.620359\n",
            "epoch:15 cost:0.620110\n",
            "epoch:16 cost:0.619922\n",
            "epoch:17 cost:0.619779\n",
            "epoch:18 cost:0.619671\n",
            "epoch:19 cost:0.619587\n",
            "epoch:20 cost:0.619522\n",
            "epoch:21 cost:0.619470\n",
            "epoch:22 cost:0.619429\n",
            "epoch:23 cost:0.619396\n",
            "epoch:24 cost:0.619368\n",
            "epoch:25 cost:0.619345\n",
            "epoch:26 cost:0.619325\n",
            "epoch:27 cost:0.619308\n",
            "epoch:28 cost:0.619292\n",
            "epoch:29 cost:0.619277\n",
            "epoch:30 cost:0.619264\n",
            "epoch:31 cost:0.619251\n",
            "epoch:32 cost:0.619239\n",
            "epoch:33 cost:0.619228\n",
            "epoch:34 cost:0.619216\n",
            "epoch:35 cost:0.619205\n",
            "epoch:36 cost:0.619194\n",
            "epoch:37 cost:0.619183\n",
            "epoch:38 cost:0.619173\n",
            "epoch:39 cost:0.619162\n",
            "epoch:40 cost:0.619152\n",
            "epoch:41 cost:0.619141\n",
            "epoch:42 cost:0.619131\n",
            "epoch:43 cost:0.619120\n",
            "epoch:44 cost:0.619110\n",
            "epoch:45 cost:0.619100\n",
            "epoch:46 cost:0.619089\n",
            "epoch:47 cost:0.619079\n",
            "epoch:48 cost:0.619069\n",
            "epoch:49 cost:0.619058\n",
            "epoch:50 cost:0.619048\n",
            "epoch:51 cost:0.619038\n",
            "epoch:52 cost:0.619027\n",
            "epoch:53 cost:0.619017\n",
            "epoch:54 cost:0.619007\n",
            "epoch:55 cost:0.618996\n",
            "epoch:56 cost:0.618986\n",
            "epoch:57 cost:0.618976\n",
            "epoch:58 cost:0.618965\n",
            "epoch:59 cost:0.618955\n",
            "epoch:60 cost:0.618945\n",
            "epoch:61 cost:0.618935\n",
            "epoch:62 cost:0.618924\n",
            "epoch:63 cost:0.618914\n",
            "epoch:64 cost:0.618904\n",
            "epoch:65 cost:0.618893\n",
            "epoch:66 cost:0.618883\n",
            "epoch:67 cost:0.618873\n",
            "epoch:68 cost:0.618863\n",
            "epoch:69 cost:0.618852\n",
            "epoch:70 cost:0.618842\n",
            "epoch:71 cost:0.618832\n",
            "epoch:72 cost:0.618821\n",
            "epoch:73 cost:0.618811\n",
            "epoch:74 cost:0.618801\n",
            "epoch:75 cost:0.618791\n",
            "epoch:76 cost:0.618780\n",
            "epoch:77 cost:0.618770\n",
            "epoch:78 cost:0.618760\n",
            "epoch:79 cost:0.618750\n",
            "epoch:80 cost:0.618739\n",
            "epoch:81 cost:0.618729\n",
            "epoch:82 cost:0.618719\n",
            "epoch:83 cost:0.618709\n",
            "epoch:84 cost:0.618698\n",
            "epoch:85 cost:0.618688\n",
            "epoch:86 cost:0.618678\n",
            "epoch:87 cost:0.618668\n",
            "epoch:88 cost:0.618657\n",
            "epoch:89 cost:0.618647\n",
            "epoch:90 cost:0.618637\n",
            "epoch:91 cost:0.618627\n",
            "epoch:92 cost:0.618617\n",
            "epoch:93 cost:0.618606\n",
            "epoch:94 cost:0.618596\n",
            "epoch:95 cost:0.618586\n",
            "epoch:96 cost:0.618576\n",
            "epoch:97 cost:0.618565\n",
            "epoch:98 cost:0.618555\n",
            "epoch:99 cost:0.618545\n",
            "\n",
            "————After train————\n",
            "W: tensor([[0.0001],\n",
            "        [0.0033]], requires_grad=True)\n",
            "b: tensor([-4.8515e-05], requires_grad=True)\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example 1\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "\n",
        "x_data = [[80,220],[75,167],[86,210],[110,330],[95,280],[67,190],[79,210],[98,250]]\n",
        "y_data = [[1],[0],[1],[1],[1],[0],[0],[1]]\n",
        "\n",
        "x_train = torch.FloatTensor(x_data)\n",
        "y_train = torch.FloatTensor(y_data)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "W = torch.zeros((2,1), requires_grad = True)\n",
        "b = torch.zeros(1, requires_grad = True)\n",
        "\n",
        "print('\\n————Before train————')\n",
        "print('W:',W)\n",
        "print('b:',b)\n",
        "print('\\n')\n",
        "\n",
        "optimizer = optim.SGD([W,b], lr = 0.00001)\n",
        "for e in range(100):\n",
        "  optimizer.zero_grad()\n",
        "  hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(W) + b)))\n",
        "  loss = -(y_train * torch.log(hypothesis) + (1 - y_train) * torch.log(1 - hypothesis))\n",
        "  cost = loss.mean()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  print('epoch:%d cost:%f'%(e, cost))\n",
        "\n",
        "\n",
        "print('\\n————After train————')\n",
        "print('W:',W)\n",
        "print('b:',b)\n",
        "print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1-1\n",
        "\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "import torch  \n",
        "import torch.nn as nn \n",
        "import torch.optim as optim \n",
        "import torch.nn.init as init \n",
        "import torch.nn.functional as F\n",
        "\n",
        "x_data = [[80,220],[75,167],[86,210],[110,330],[95,280],[67,190],[79,210],[98,250]] \n",
        "y_data = [[1],[0],[1],[1],[1],[0],[0],[1]]\n",
        "\n",
        "x_train = torch.FloatTensor(x_data) \n",
        "y_train = torch.FloatTensor(y_data)\n",
        "\n",
        "\n",
        "print(x_train.shape) \n",
        "print(y_train.shape)\n",
        "W = torch.zeros((2,1), requires_grad = True) \n",
        "b = torch.zeros(1, requires_grad = True)\n",
        "optimizer = optim.SGD([W,b], lr = 0.00001)\n",
        "\n",
        "for e in range(100): \n",
        "  optimizer.zero_grad() \n",
        "\n",
        "  # Use hypothesis directly\n",
        "  hypothesis = torch.sigmoid(x_train.matmul(W)+b) \n",
        "  cost = F.binary_cross_entropy(hypothesis, y_train) \n",
        "  cost.backward() \n",
        "  optimizer.step()\n",
        "\n",
        "  print('epoch:%d cost:%f'%(e, cost))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FG4GdoAE9Lyv",
        "outputId": "4f0f4082-22df-4b71-ae2f-4181f4bc7c83"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 2])\n",
            "torch.Size([8, 1])\n",
            "epoch:0 cost:0.693147\n",
            "epoch:1 cost:0.672145\n",
            "epoch:2 cost:0.657294\n",
            "epoch:3 cost:0.646757\n",
            "epoch:4 cost:0.639247\n",
            "epoch:5 cost:0.633865\n",
            "epoch:6 cost:0.629987\n",
            "epoch:7 cost:0.627179\n",
            "epoch:8 cost:0.625135\n",
            "epoch:9 cost:0.623641\n",
            "epoch:10 cost:0.622544\n",
            "epoch:11 cost:0.621735\n",
            "epoch:12 cost:0.621136\n",
            "epoch:13 cost:0.620691\n",
            "epoch:14 cost:0.620359\n",
            "epoch:15 cost:0.620110\n",
            "epoch:16 cost:0.619922\n",
            "epoch:17 cost:0.619779\n",
            "epoch:18 cost:0.619671\n",
            "epoch:19 cost:0.619587\n",
            "epoch:20 cost:0.619522\n",
            "epoch:21 cost:0.619470\n",
            "epoch:22 cost:0.619429\n",
            "epoch:23 cost:0.619396\n",
            "epoch:24 cost:0.619368\n",
            "epoch:25 cost:0.619345\n",
            "epoch:26 cost:0.619325\n",
            "epoch:27 cost:0.619308\n",
            "epoch:28 cost:0.619292\n",
            "epoch:29 cost:0.619277\n",
            "epoch:30 cost:0.619264\n",
            "epoch:31 cost:0.619251\n",
            "epoch:32 cost:0.619239\n",
            "epoch:33 cost:0.619228\n",
            "epoch:34 cost:0.619216\n",
            "epoch:35 cost:0.619205\n",
            "epoch:36 cost:0.619194\n",
            "epoch:37 cost:0.619183\n",
            "epoch:38 cost:0.619173\n",
            "epoch:39 cost:0.619162\n",
            "epoch:40 cost:0.619152\n",
            "epoch:41 cost:0.619141\n",
            "epoch:42 cost:0.619131\n",
            "epoch:43 cost:0.619120\n",
            "epoch:44 cost:0.619110\n",
            "epoch:45 cost:0.619100\n",
            "epoch:46 cost:0.619089\n",
            "epoch:47 cost:0.619079\n",
            "epoch:48 cost:0.619069\n",
            "epoch:49 cost:0.619058\n",
            "epoch:50 cost:0.619048\n",
            "epoch:51 cost:0.619038\n",
            "epoch:52 cost:0.619027\n",
            "epoch:53 cost:0.619017\n",
            "epoch:54 cost:0.619007\n",
            "epoch:55 cost:0.618996\n",
            "epoch:56 cost:0.618986\n",
            "epoch:57 cost:0.618976\n",
            "epoch:58 cost:0.618965\n",
            "epoch:59 cost:0.618955\n",
            "epoch:60 cost:0.618945\n",
            "epoch:61 cost:0.618935\n",
            "epoch:62 cost:0.618924\n",
            "epoch:63 cost:0.618914\n",
            "epoch:64 cost:0.618904\n",
            "epoch:65 cost:0.618893\n",
            "epoch:66 cost:0.618883\n",
            "epoch:67 cost:0.618873\n",
            "epoch:68 cost:0.618863\n",
            "epoch:69 cost:0.618852\n",
            "epoch:70 cost:0.618842\n",
            "epoch:71 cost:0.618832\n",
            "epoch:72 cost:0.618821\n",
            "epoch:73 cost:0.618811\n",
            "epoch:74 cost:0.618801\n",
            "epoch:75 cost:0.618791\n",
            "epoch:76 cost:0.618780\n",
            "epoch:77 cost:0.618770\n",
            "epoch:78 cost:0.618760\n",
            "epoch:79 cost:0.618750\n",
            "epoch:80 cost:0.618739\n",
            "epoch:81 cost:0.618729\n",
            "epoch:82 cost:0.618719\n",
            "epoch:83 cost:0.618709\n",
            "epoch:84 cost:0.618698\n",
            "epoch:85 cost:0.618688\n",
            "epoch:86 cost:0.618678\n",
            "epoch:87 cost:0.618668\n",
            "epoch:88 cost:0.618657\n",
            "epoch:89 cost:0.618647\n",
            "epoch:90 cost:0.618637\n",
            "epoch:91 cost:0.618627\n",
            "epoch:92 cost:0.618617\n",
            "epoch:93 cost:0.618606\n",
            "epoch:94 cost:0.618596\n",
            "epoch:95 cost:0.618586\n",
            "epoch:96 cost:0.618576\n",
            "epoch:97 cost:0.618566\n",
            "epoch:98 cost:0.618555\n",
            "epoch:99 cost:0.618545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2  Multi-Class Classification\n",
        "\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "import torch \n",
        "import torch.nn as nn \n",
        "import torch.optim as optim \n",
        "import torch.nn.init as init \n",
        "import torch.nn.functional as F\n",
        "\n",
        "x_data = [[80,220,6300],\n",
        "          [75,167,4500],\n",
        "          [86,210,7500],\n",
        "          [110,330,9000],\n",
        "          [95,280,8700],\n",
        "          [67,190,6800],\n",
        "          [79,210,5000],\n",
        "          [98,250,7200]]\n",
        "y_data = [2,3,1,0,0,3,2,1]  # GT is 1D tensor\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "# Transform as tensor formation\n",
        "x_train = torch.FloatTensor(x_data)\n",
        "y_train = torch.LongTensor(y_data)  # Convert label to Long Tensor\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "W = torch.zeros((3,4), requires_grad = True)\n",
        "b = torch.zeros(1,requires_grad = True)\n",
        "print('\\n--------Before train--------')\n",
        "print('W size:',W)\n",
        "print('b size:',b)\n",
        "print(\"\")\n",
        "\n",
        "optimizer = optim.SGD([W,b], lr = 0.00001)\n",
        "\n",
        "for e in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    z = torch.sigmoid(x_train.matmul(W) + b)\n",
        "    cost = F.cross_entropy(z,y_train)\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "    print(\"epoch: %d cost: %f\" %(e,cost))\n",
        "\n",
        "print('\\n————After train————')\n",
        "print('W size:',W)\n",
        "print('b size:',b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECnjVHzA-Uxd",
        "outputId": "459093df-5397-4124-ca05-b27f4b025a63"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 3])\n",
            "torch.Size([8])\n",
            "torch.Size([8, 3])\n",
            "torch.Size([8])\n",
            "\n",
            "--------Before train--------\n",
            "W size: tensor([[0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.]], requires_grad=True)\n",
            "b size: tensor([0.], requires_grad=True)\n",
            "\n",
            "epoch: 0 cost: 1.386294\n",
            "epoch: 1 cost: 1.483104\n",
            "epoch: 2 cost: 1.481087\n",
            "epoch: 3 cost: 1.451981\n",
            "epoch: 4 cost: 1.450405\n",
            "epoch: 5 cost: 1.458136\n",
            "epoch: 6 cost: 1.393971\n",
            "epoch: 7 cost: 1.387187\n",
            "epoch: 8 cost: 1.384839\n",
            "epoch: 9 cost: 1.383867\n",
            "epoch: 10 cost: 1.383315\n",
            "epoch: 11 cost: 1.382935\n",
            "epoch: 12 cost: 1.382634\n",
            "epoch: 13 cost: 1.382365\n",
            "epoch: 14 cost: 1.382093\n",
            "epoch: 15 cost: 1.381780\n",
            "epoch: 16 cost: 1.381363\n",
            "epoch: 17 cost: 1.380711\n",
            "epoch: 18 cost: 1.379455\n",
            "epoch: 19 cost: 1.376300\n",
            "epoch: 20 cost: 1.369258\n",
            "epoch: 21 cost: 1.369208\n",
            "epoch: 22 cost: 1.369384\n",
            "epoch: 23 cost: 1.373797\n",
            "epoch: 24 cost: 1.382478\n",
            "epoch: 25 cost: 1.382247\n",
            "epoch: 26 cost: 1.382006\n",
            "epoch: 27 cost: 1.381938\n",
            "epoch: 28 cost: 1.381861\n",
            "epoch: 29 cost: 1.381769\n",
            "epoch: 30 cost: 1.381656\n",
            "epoch: 31 cost: 1.381508\n",
            "epoch: 32 cost: 1.381305\n",
            "epoch: 33 cost: 1.381003\n",
            "epoch: 34 cost: 1.380504\n",
            "epoch: 35 cost: 1.379554\n",
            "epoch: 36 cost: 1.377298\n",
            "epoch: 37 cost: 1.370867\n",
            "epoch: 38 cost: 1.387495\n",
            "epoch: 39 cost: 1.383205\n",
            "epoch: 40 cost: 1.382905\n",
            "epoch: 41 cost: 1.382325\n",
            "epoch: 42 cost: 1.382247\n",
            "epoch: 43 cost: 1.382239\n",
            "epoch: 44 cost: 1.382232\n",
            "epoch: 45 cost: 1.382225\n",
            "epoch: 46 cost: 1.382219\n",
            "epoch: 47 cost: 1.382213\n",
            "epoch: 48 cost: 1.382207\n",
            "epoch: 49 cost: 1.382202\n",
            "epoch: 50 cost: 1.382196\n",
            "epoch: 51 cost: 1.382191\n",
            "epoch: 52 cost: 1.382186\n",
            "epoch: 53 cost: 1.382181\n",
            "epoch: 54 cost: 1.382176\n",
            "epoch: 55 cost: 1.382172\n",
            "epoch: 56 cost: 1.382167\n",
            "epoch: 57 cost: 1.382163\n",
            "epoch: 58 cost: 1.382159\n",
            "epoch: 59 cost: 1.382155\n",
            "epoch: 60 cost: 1.382151\n",
            "epoch: 61 cost: 1.382147\n",
            "epoch: 62 cost: 1.382143\n",
            "epoch: 63 cost: 1.382140\n",
            "epoch: 64 cost: 1.382136\n",
            "epoch: 65 cost: 1.382133\n",
            "epoch: 66 cost: 1.382129\n",
            "epoch: 67 cost: 1.382126\n",
            "epoch: 68 cost: 1.382123\n",
            "epoch: 69 cost: 1.382120\n",
            "epoch: 70 cost: 1.382116\n",
            "epoch: 71 cost: 1.382113\n",
            "epoch: 72 cost: 1.382110\n",
            "epoch: 73 cost: 1.382107\n",
            "epoch: 74 cost: 1.382104\n",
            "epoch: 75 cost: 1.382102\n",
            "epoch: 76 cost: 1.382099\n",
            "epoch: 77 cost: 1.382096\n",
            "epoch: 78 cost: 1.382093\n",
            "epoch: 79 cost: 1.382091\n",
            "epoch: 80 cost: 1.382088\n",
            "epoch: 81 cost: 1.382085\n",
            "epoch: 82 cost: 1.382083\n",
            "epoch: 83 cost: 1.382080\n",
            "epoch: 84 cost: 1.382078\n",
            "epoch: 85 cost: 1.382075\n",
            "epoch: 86 cost: 1.382073\n",
            "epoch: 87 cost: 1.382071\n",
            "epoch: 88 cost: 1.382068\n",
            "epoch: 89 cost: 1.382066\n",
            "epoch: 90 cost: 1.382064\n",
            "epoch: 91 cost: 1.382062\n",
            "epoch: 92 cost: 1.382059\n",
            "epoch: 93 cost: 1.382057\n",
            "epoch: 94 cost: 1.382055\n",
            "epoch: 95 cost: 1.382053\n",
            "epoch: 96 cost: 1.382050\n",
            "epoch: 97 cost: 1.382048\n",
            "epoch: 98 cost: 1.382046\n",
            "epoch: 99 cost: 1.382044\n",
            "\n",
            "————After train————\n",
            "W size: tensor([[-9.1994e-07, -6.1644e-05,  3.5382e-05,  2.8474e-05],\n",
            "        [ 7.7833e-05, -3.3605e-04,  1.0410e-04,  3.7370e-05],\n",
            "        [ 2.1573e-03,  3.8028e-04,  1.4164e-03,  1.6131e-03]],\n",
            "       requires_grad=True)\n",
            "b size: tensor([-1.8576e-06], requires_grad=True)\n"
          ]
        }
      ]
    }
  ]
}