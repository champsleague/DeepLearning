{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/champsleague/DeepLearning/blob/main/DL_Lab08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "chars = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "char_list = [i for i in chars]\n",
        "n_letters = len(char_list)\n",
        "\n",
        "n_layers = 1\n",
        "\n",
        "five_words = ['basic','beach','below','black','brown','carry','cream','drink','error','event','exist','first','funny','guess','human','image','large','magic','mouse','night','noise','ocean','often','order','peace','phone','print','quiet','reach','rough','round','scene','score','sense','skill','sleep','small','storm','table','think','touch','twice','until','upset','voice','waste','watch','white','woman','young']\n",
        "n_five_words = len(five_words)\n",
        "\n",
        "sequence_length = 4\n",
        "\n",
        "def word_to_onehot(string):\n",
        "    one_hot = np.array([]).reshape(0,n_letters)\n",
        "    for i in string:\n",
        "      idx = char_list.index(i)\n",
        "      zero = np.zeros(shape=n_letters, dtype=int)\n",
        "      zero[idx] = 1\n",
        "      one_hot = np.vstack([one_hot, zero])\n",
        "    return one_hot\n",
        "\n",
        "def onehot_to_word(onehot_1):\n",
        "    onehot = torch.Tensor.numpy(onehot_1)\n",
        "    return char_list[onehot.argmax()]\n",
        "\n",
        "# Use RNN Packages \n",
        "class myRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layer):\n",
        "    super(myRNN,  self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layer = num_layer\n",
        "    \n",
        "    self.rnn = nn.RNN(input_size = input_size,hidden_size=hidden_size, num_layers=num_layer)\n",
        "    \n",
        "  def forward(self, x, hidden):\n",
        "    out, hidden = self.rnn(x, hidden)\n",
        "    return out, hidden\n",
        "    \n",
        "  def init_hidden(self):\n",
        "    return torch.zeros(self.num_layer, 1, self.hidden_size)\n",
        "\n",
        "\n",
        "def main():\n",
        "  n_hidden = 26\n",
        "  lr = 0.001\n",
        "  epochs = 900\n",
        "  \n",
        "  model = myRNN(n_letters, n_hidden, n_layers)\n",
        "  \n",
        "  loss_func = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 300, gamma=0.1)\n",
        "  \n",
        "  for i in range(epochs):\n",
        "    total_loss = 0\n",
        "    for j in range(n_five_words):\n",
        "      hidden = model.init_hidden()\n",
        "      string = five_words[j]\n",
        "      one_hot = torch.from_numpy(word_to_onehot(string)).type_as(torch.FloatTensor())\n",
        "      model.zero_grad()\n",
        "      hidden = model.init_hidden()\n",
        "      input = one_hot[0:-1]\n",
        "      input = torch.unsqueeze(input, 1)\n",
        "      target = np.argmax(one_hot[1:], axis=1)\n",
        "      \n",
        "      output, hidden  = model(input, hidden)\n",
        "      \n",
        "      loss = loss_func(output.squeeze(1), target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "    if i%10 == 0:\n",
        "      print('epoch%d'%i)\n",
        "      print(loss)\n",
        "      \n",
        "    scheduler.step()\n",
        "    \n",
        "  torch.save(model.state_dict(), 'trained.pth')\n",
        "  model.load_state_dict(torch.load('trained.pth'))\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    total = 0\n",
        "    positive = 0\n",
        "    total_text = 0\n",
        "    positive_text = 0\n",
        "    for i in range(n_five_words):\n",
        "      string = five_words[i]\n",
        "      one_hot = torch.from_numpy(word_to_onehot(string)).type_as(torch.FloatTensor())\n",
        "      hidden = model.init_hidden()\n",
        "      input = one_hot[0:-1]\n",
        "      input = torch.unsqueeze(input, 1)\n",
        "      target = np.argmax(one_hot[1:], axis=1)\n",
        "      output, hidden = model(input, hidden)\n",
        "      output = output.squeeze()\n",
        "      \n",
        "      output_string = string[0]\n",
        "      \n",
        "      for j in range(output.size()[0]):\n",
        "        output_string += onehot_to_word(output[j].data)\n",
        "        total_text += 1\n",
        "        \n",
        "        if string[j+1] == output_string[-1]:\n",
        "          positive_text += 1\n",
        "          \n",
        "      total += 1\n",
        "      if string[-1] == output_string[-1]:\n",
        "        positive += 1\n",
        "        \n",
        "      print('%d GT:%s OUT:%s'%(i+1, string, output_string))\n",
        "      \n",
        "    print('final text accuracy %d/%d (%.4f)'%(positive, total, positive/total))\n",
        "    print('whole text accuracy %d/%d (%.4f)' % (positive_text, total_text, positive_text / total_text))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czRhsYzaDBq_",
        "outputId": "f1abb916-f1dc-45e0-c167-a0eebae81ec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch0\n",
            "tensor(3.2299, grad_fn=<NllLossBackward0>)\n",
            "epoch10\n",
            "tensor(2.8090, grad_fn=<NllLossBackward0>)\n",
            "epoch20\n",
            "tensor(2.6459, grad_fn=<NllLossBackward0>)\n",
            "epoch30\n",
            "tensor(2.5552, grad_fn=<NllLossBackward0>)\n",
            "epoch40\n",
            "tensor(2.4358, grad_fn=<NllLossBackward0>)\n",
            "epoch50\n",
            "tensor(2.3017, grad_fn=<NllLossBackward0>)\n",
            "epoch60\n",
            "tensor(2.2103, grad_fn=<NllLossBackward0>)\n",
            "epoch70\n",
            "tensor(2.1498, grad_fn=<NllLossBackward0>)\n",
            "epoch80\n",
            "tensor(2.1044, grad_fn=<NllLossBackward0>)\n",
            "epoch90\n",
            "tensor(2.0612, grad_fn=<NllLossBackward0>)\n",
            "epoch100\n",
            "tensor(2.0069, grad_fn=<NllLossBackward0>)\n",
            "epoch110\n",
            "tensor(1.9556, grad_fn=<NllLossBackward0>)\n",
            "epoch120\n",
            "tensor(1.9199, grad_fn=<NllLossBackward0>)\n",
            "epoch130\n",
            "tensor(1.8941, grad_fn=<NllLossBackward0>)\n",
            "epoch140\n",
            "tensor(1.8731, grad_fn=<NllLossBackward0>)\n",
            "epoch150\n",
            "tensor(1.8551, grad_fn=<NllLossBackward0>)\n",
            "epoch160\n",
            "tensor(1.8406, grad_fn=<NllLossBackward0>)\n",
            "epoch170\n",
            "tensor(1.8277, grad_fn=<NllLossBackward0>)\n",
            "epoch180\n",
            "tensor(1.8164, grad_fn=<NllLossBackward0>)\n",
            "epoch190\n",
            "tensor(1.8062, grad_fn=<NllLossBackward0>)\n",
            "epoch200\n",
            "tensor(1.7966, grad_fn=<NllLossBackward0>)\n",
            "epoch210\n",
            "tensor(1.7870, grad_fn=<NllLossBackward0>)\n",
            "epoch220\n",
            "tensor(1.7774, grad_fn=<NllLossBackward0>)\n",
            "epoch230\n",
            "tensor(1.7678, grad_fn=<NllLossBackward0>)\n",
            "epoch240\n",
            "tensor(1.7582, grad_fn=<NllLossBackward0>)\n",
            "epoch250\n",
            "tensor(1.7493, grad_fn=<NllLossBackward0>)\n",
            "epoch260\n",
            "tensor(1.7401, grad_fn=<NllLossBackward0>)\n",
            "epoch270\n",
            "tensor(1.7328, grad_fn=<NllLossBackward0>)\n",
            "epoch280\n",
            "tensor(1.7226, grad_fn=<NllLossBackward0>)\n",
            "epoch290\n",
            "tensor(1.7142, grad_fn=<NllLossBackward0>)\n",
            "epoch300\n",
            "tensor(1.7070, grad_fn=<NllLossBackward0>)\n",
            "epoch310\n",
            "tensor(1.7066, grad_fn=<NllLossBackward0>)\n",
            "epoch320\n",
            "tensor(1.7056, grad_fn=<NllLossBackward0>)\n",
            "epoch330\n",
            "tensor(1.7046, grad_fn=<NllLossBackward0>)\n",
            "epoch340\n",
            "tensor(1.7035, grad_fn=<NllLossBackward0>)\n",
            "epoch350\n",
            "tensor(1.7025, grad_fn=<NllLossBackward0>)\n",
            "epoch360\n",
            "tensor(1.7014, grad_fn=<NllLossBackward0>)\n",
            "epoch370\n",
            "tensor(1.7003, grad_fn=<NllLossBackward0>)\n",
            "epoch380\n",
            "tensor(1.6992, grad_fn=<NllLossBackward0>)\n",
            "epoch390\n",
            "tensor(1.6981, grad_fn=<NllLossBackward0>)\n",
            "epoch400\n",
            "tensor(1.6970, grad_fn=<NllLossBackward0>)\n",
            "epoch410\n",
            "tensor(1.6959, grad_fn=<NllLossBackward0>)\n",
            "epoch420\n",
            "tensor(1.6949, grad_fn=<NllLossBackward0>)\n",
            "epoch430\n",
            "tensor(1.6938, grad_fn=<NllLossBackward0>)\n",
            "epoch440\n",
            "tensor(1.6928, grad_fn=<NllLossBackward0>)\n",
            "epoch450\n",
            "tensor(1.6918, grad_fn=<NllLossBackward0>)\n",
            "epoch460\n",
            "tensor(1.6909, grad_fn=<NllLossBackward0>)\n",
            "epoch470\n",
            "tensor(1.6899, grad_fn=<NllLossBackward0>)\n",
            "epoch480\n",
            "tensor(1.6890, grad_fn=<NllLossBackward0>)\n",
            "epoch490\n",
            "tensor(1.6881, grad_fn=<NllLossBackward0>)\n",
            "epoch500\n",
            "tensor(1.6872, grad_fn=<NllLossBackward0>)\n",
            "epoch510\n",
            "tensor(1.6863, grad_fn=<NllLossBackward0>)\n",
            "epoch520\n",
            "tensor(1.6854, grad_fn=<NllLossBackward0>)\n",
            "epoch530\n",
            "tensor(1.6845, grad_fn=<NllLossBackward0>)\n",
            "epoch540\n",
            "tensor(1.6837, grad_fn=<NllLossBackward0>)\n",
            "epoch550\n",
            "tensor(1.6828, grad_fn=<NllLossBackward0>)\n",
            "epoch560\n",
            "tensor(1.6819, grad_fn=<NllLossBackward0>)\n",
            "epoch570\n",
            "tensor(1.6810, grad_fn=<NllLossBackward0>)\n",
            "epoch580\n",
            "tensor(1.6802, grad_fn=<NllLossBackward0>)\n",
            "epoch590\n",
            "tensor(1.6794, grad_fn=<NllLossBackward0>)\n",
            "epoch600\n",
            "tensor(1.6786, grad_fn=<NllLossBackward0>)\n",
            "epoch610\n",
            "tensor(1.6784, grad_fn=<NllLossBackward0>)\n",
            "epoch620\n",
            "tensor(1.6783, grad_fn=<NllLossBackward0>)\n",
            "epoch630\n",
            "tensor(1.6783, grad_fn=<NllLossBackward0>)\n",
            "epoch640\n",
            "tensor(1.6782, grad_fn=<NllLossBackward0>)\n",
            "epoch650\n",
            "tensor(1.6781, grad_fn=<NllLossBackward0>)\n",
            "epoch660\n",
            "tensor(1.6780, grad_fn=<NllLossBackward0>)\n",
            "epoch670\n",
            "tensor(1.6779, grad_fn=<NllLossBackward0>)\n",
            "epoch680\n",
            "tensor(1.6778, grad_fn=<NllLossBackward0>)\n",
            "epoch690\n",
            "tensor(1.6777, grad_fn=<NllLossBackward0>)\n",
            "epoch700\n",
            "tensor(1.6776, grad_fn=<NllLossBackward0>)\n",
            "epoch710\n",
            "tensor(1.6775, grad_fn=<NllLossBackward0>)\n",
            "epoch720\n",
            "tensor(1.6774, grad_fn=<NllLossBackward0>)\n",
            "epoch730\n",
            "tensor(1.6773, grad_fn=<NllLossBackward0>)\n",
            "epoch740\n",
            "tensor(1.6773, grad_fn=<NllLossBackward0>)\n",
            "epoch750\n",
            "tensor(1.6772, grad_fn=<NllLossBackward0>)\n",
            "epoch760\n",
            "tensor(1.6771, grad_fn=<NllLossBackward0>)\n",
            "epoch770\n",
            "tensor(1.6770, grad_fn=<NllLossBackward0>)\n",
            "epoch780\n",
            "tensor(1.6769, grad_fn=<NllLossBackward0>)\n",
            "epoch790\n",
            "tensor(1.6768, grad_fn=<NllLossBackward0>)\n",
            "epoch800\n",
            "tensor(1.6767, grad_fn=<NllLossBackward0>)\n",
            "epoch810\n",
            "tensor(1.6766, grad_fn=<NllLossBackward0>)\n",
            "epoch820\n",
            "tensor(1.6766, grad_fn=<NllLossBackward0>)\n",
            "epoch830\n",
            "tensor(1.6765, grad_fn=<NllLossBackward0>)\n",
            "epoch840\n",
            "tensor(1.6764, grad_fn=<NllLossBackward0>)\n",
            "epoch850\n",
            "tensor(1.6763, grad_fn=<NllLossBackward0>)\n",
            "epoch860\n",
            "tensor(1.6762, grad_fn=<NllLossBackward0>)\n",
            "epoch870\n",
            "tensor(1.6761, grad_fn=<NllLossBackward0>)\n",
            "epoch880\n",
            "tensor(1.6760, grad_fn=<NllLossBackward0>)\n",
            "epoch890\n",
            "tensor(1.6760, grad_fn=<NllLossBackward0>)\n",
            "1 GT:basic OUT:blsic\n",
            "2 GT:beach OUT:blsch\n",
            "3 GT:below OUT:blsow\n",
            "4 GT:black OUT:blagh\n",
            "5 GT:brown OUT:blswt\n",
            "6 GT:carry OUT:crrry\n",
            "7 GT:cream OUT:crram\n",
            "8 GT:drink OUT:drink\n",
            "9 GT:error OUT:eaeor\n",
            "10 GT:event OUT:eaent\n",
            "11 GT:exist OUT:eaist\n",
            "12 GT:first OUT:fucst\n",
            "13 GT:funny OUT:funny\n",
            "14 GT:guess OUT:guess\n",
            "15 GT:human OUT:homan\n",
            "16 GT:image OUT:icage\n",
            "17 GT:large OUT:large\n",
            "18 GT:magic OUT:manic\n",
            "19 GT:mouse OUT:mause\n",
            "20 GT:night OUT:noiht\n",
            "21 GT:noise OUT:noise\n",
            "22 GT:ocean OUT:orean\n",
            "23 GT:often OUT:orten\n",
            "24 GT:order OUT:orden\n",
            "25 GT:peace OUT:prane\n",
            "26 GT:phone OUT:prone\n",
            "27 GT:print OUT:pront\n",
            "28 GT:quiet OUT:quiee\n",
            "29 GT:reach OUT:roach\n",
            "30 GT:rough OUT:rounh\n",
            "31 GT:round OUT:rounk\n",
            "32 GT:scene OUT:seene\n",
            "33 GT:score OUT:seere\n",
            "34 GT:sense OUT:setee\n",
            "35 GT:skill OUT:seill\n",
            "36 GT:sleep OUT:seoep\n",
            "37 GT:small OUT:seonl\n",
            "38 GT:storm OUT:seorm\n",
            "39 GT:table OUT:toble\n",
            "40 GT:think OUT:toick\n",
            "41 GT:touch OUT:toinh\n",
            "42 GT:twice OUT:toice\n",
            "43 GT:until OUT:upten\n",
            "44 GT:upset OUT:upsee\n",
            "45 GT:voice OUT:voice\n",
            "46 GT:waste OUT:wamee\n",
            "47 GT:watch OUT:wameh\n",
            "48 GT:white OUT:waiee\n",
            "49 GT:woman OUT:waicn\n",
            "50 GT:young OUT:young\n",
            "final text accuracy 43/50 (0.8600)\n",
            "whole text accuracy 134/200 (0.6700)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPuUBNJtQJYv",
        "outputId": "60a2bb76-017c-4885-8492-b887807368b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch0\n",
            "tensor(3.2802, grad_fn=<NllLossBackward0>)\n",
            "epoch10\n",
            "tensor(2.5830, grad_fn=<NllLossBackward0>)\n",
            "epoch20\n",
            "tensor(2.4084, grad_fn=<NllLossBackward0>)\n",
            "epoch30\n",
            "tensor(2.1654, grad_fn=<NllLossBackward0>)\n",
            "epoch40\n",
            "tensor(2.0141, grad_fn=<NllLossBackward0>)\n",
            "epoch50\n",
            "tensor(1.9203, grad_fn=<NllLossBackward0>)\n",
            "epoch60\n",
            "tensor(1.8524, grad_fn=<NllLossBackward0>)\n",
            "epoch70\n",
            "tensor(1.7906, grad_fn=<NllLossBackward0>)\n",
            "epoch80\n",
            "tensor(1.7311, grad_fn=<NllLossBackward0>)\n",
            "epoch90\n",
            "tensor(1.6833, grad_fn=<NllLossBackward0>)\n",
            "epoch100\n",
            "tensor(1.6532, grad_fn=<NllLossBackward0>)\n",
            "epoch110\n",
            "tensor(1.6163, grad_fn=<NllLossBackward0>)\n",
            "epoch120\n",
            "tensor(1.5561, grad_fn=<NllLossBackward0>)\n",
            "epoch130\n",
            "tensor(1.5378, grad_fn=<NllLossBackward0>)\n",
            "epoch140\n",
            "tensor(1.5297, grad_fn=<NllLossBackward0>)\n",
            "epoch150\n",
            "tensor(1.5243, grad_fn=<NllLossBackward0>)\n",
            "epoch160\n",
            "tensor(1.5202, grad_fn=<NllLossBackward0>)\n",
            "epoch170\n",
            "tensor(1.5177, grad_fn=<NllLossBackward0>)\n",
            "epoch180\n",
            "tensor(1.5156, grad_fn=<NllLossBackward0>)\n",
            "epoch190\n",
            "tensor(1.5140, grad_fn=<NllLossBackward0>)\n",
            "epoch200\n",
            "tensor(1.5133, grad_fn=<NllLossBackward0>)\n",
            "epoch210\n",
            "tensor(1.5121, grad_fn=<NllLossBackward0>)\n",
            "epoch220\n",
            "tensor(1.5113, grad_fn=<NllLossBackward0>)\n",
            "epoch230\n",
            "tensor(1.5107, grad_fn=<NllLossBackward0>)\n",
            "epoch240\n",
            "tensor(1.5133, grad_fn=<NllLossBackward0>)\n",
            "epoch250\n",
            "tensor(1.5106, grad_fn=<NllLossBackward0>)\n",
            "epoch260\n",
            "tensor(1.5098, grad_fn=<NllLossBackward0>)\n",
            "epoch270\n",
            "tensor(1.5095, grad_fn=<NllLossBackward0>)\n",
            "epoch280\n",
            "tensor(1.5094, grad_fn=<NllLossBackward0>)\n",
            "epoch290\n",
            "tensor(1.5091, grad_fn=<NllLossBackward0>)\n",
            "epoch300\n",
            "tensor(1.5090, grad_fn=<NllLossBackward0>)\n",
            "epoch310\n",
            "tensor(1.5090, grad_fn=<NllLossBackward0>)\n",
            "epoch320\n",
            "tensor(1.5089, grad_fn=<NllLossBackward0>)\n",
            "epoch330\n",
            "tensor(1.5089, grad_fn=<NllLossBackward0>)\n",
            "epoch340\n",
            "tensor(1.5089, grad_fn=<NllLossBackward0>)\n",
            "epoch350\n",
            "tensor(1.5089, grad_fn=<NllLossBackward0>)\n",
            "epoch360\n",
            "tensor(1.5089, grad_fn=<NllLossBackward0>)\n",
            "epoch370\n",
            "tensor(1.5088, grad_fn=<NllLossBackward0>)\n",
            "epoch380\n",
            "tensor(1.5088, grad_fn=<NllLossBackward0>)\n",
            "epoch390\n",
            "tensor(1.5088, grad_fn=<NllLossBackward0>)\n",
            "epoch400\n",
            "tensor(1.5088, grad_fn=<NllLossBackward0>)\n",
            "epoch410\n",
            "tensor(1.5087, grad_fn=<NllLossBackward0>)\n",
            "epoch420\n",
            "tensor(1.5087, grad_fn=<NllLossBackward0>)\n",
            "epoch430\n",
            "tensor(1.5087, grad_fn=<NllLossBackward0>)\n",
            "epoch440\n",
            "tensor(1.5087, grad_fn=<NllLossBackward0>)\n",
            "epoch450\n",
            "tensor(1.5087, grad_fn=<NllLossBackward0>)\n",
            "epoch460\n",
            "tensor(1.5086, grad_fn=<NllLossBackward0>)\n",
            "epoch470\n",
            "tensor(1.5086, grad_fn=<NllLossBackward0>)\n",
            "epoch480\n",
            "tensor(1.5086, grad_fn=<NllLossBackward0>)\n",
            "epoch490\n",
            "tensor(1.5086, grad_fn=<NllLossBackward0>)\n",
            "epoch500\n",
            "tensor(1.5086, grad_fn=<NllLossBackward0>)\n",
            "epoch510\n",
            "tensor(1.5085, grad_fn=<NllLossBackward0>)\n",
            "epoch520\n",
            "tensor(1.5085, grad_fn=<NllLossBackward0>)\n",
            "epoch530\n",
            "tensor(1.5085, grad_fn=<NllLossBackward0>)\n",
            "epoch540\n",
            "tensor(1.5085, grad_fn=<NllLossBackward0>)\n",
            "epoch550\n",
            "tensor(1.5085, grad_fn=<NllLossBackward0>)\n",
            "epoch560\n",
            "tensor(1.5085, grad_fn=<NllLossBackward0>)\n",
            "epoch570\n",
            "tensor(1.5085, grad_fn=<NllLossBackward0>)\n",
            "epoch580\n",
            "tensor(1.5085, grad_fn=<NllLossBackward0>)\n",
            "epoch590\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch600\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch610\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch620\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch630\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch640\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch650\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch660\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch670\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch680\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch690\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch700\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch710\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch720\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch730\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch740\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch750\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch760\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch770\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch780\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch790\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch800\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch810\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch820\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch830\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch840\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch850\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch860\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch870\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch880\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch890\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "1 GT:basic OUT:besic\n",
            "2 GT:beach OUT:beach\n",
            "3 GT:below OUT:beaoh\n",
            "4 GT:black OUT:beack\n",
            "5 GT:brown OUT:beoun\n",
            "6 GT:carry OUT:crrry\n",
            "7 GT:cream OUT:cream\n",
            "8 GT:drink OUT:drink\n",
            "9 GT:error OUT:exeor\n",
            "10 GT:event OUT:exent\n",
            "11 GT:exist OUT:exist\n",
            "12 GT:first OUT:furst\n",
            "13 GT:funny OUT:funsy\n",
            "14 GT:guess OUT:guest\n",
            "15 GT:human OUT:honan\n",
            "16 GT:image OUT:image\n",
            "17 GT:large OUT:large\n",
            "18 GT:magic OUT:magic\n",
            "19 GT:mouse OUT:mause\n",
            "20 GT:night OUT:noiht\n",
            "21 GT:noise OUT:noise\n",
            "22 GT:ocean OUT:orean\n",
            "23 GT:often OUT:orten\n",
            "24 GT:order OUT:oreer\n",
            "25 GT:peace OUT:prace\n",
            "26 GT:phone OUT:prone\n",
            "27 GT:print OUT:print\n",
            "28 GT:quiet OUT:quiet\n",
            "29 GT:reach OUT:roach\n",
            "30 GT:rough OUT:rounh\n",
            "31 GT:round OUT:round\n",
            "32 GT:scene OUT:scene\n",
            "33 GT:score OUT:scere\n",
            "34 GT:sense OUT:scnee\n",
            "35 GT:skill OUT:scill\n",
            "36 GT:sleep OUT:sceet\n",
            "37 GT:small OUT:scoll\n",
            "38 GT:storm OUT:scorm\n",
            "39 GT:table OUT:table\n",
            "40 GT:think OUT:taink\n",
            "41 GT:touch OUT:tauch\n",
            "42 GT:twice OUT:taice\n",
            "43 GT:until OUT:upsil\n",
            "44 GT:upset OUT:upset\n",
            "45 GT:voice OUT:voice\n",
            "46 GT:waste OUT:wasce\n",
            "47 GT:watch OUT:wasch\n",
            "48 GT:white OUT:waite\n",
            "49 GT:woman OUT:waman\n",
            "50 GT:young OUT:young\n",
            "final text accuracy 47/50 (0.9400)\n",
            "whole text accuracy 157/200 (0.7850)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "chars = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "char_list = [i for i in chars]\n",
        "n_letters = len(char_list)\n",
        "\n",
        "n_layers = 2\n",
        "\n",
        "five_words = ['basic','beach','below','black','brown','carry','cream','drink','error','event','exist','first','funny','guess','human','image','large','magic','mouse','night','noise','ocean','often','order','peace','phone','print','quiet','reach','rough','round','scene','score','sense','skill','sleep','small','storm','table','think','touch','twice','until','upset','voice','waste','watch','white','woman','young']\n",
        "n_five_words = len(five_words)\n",
        "\n",
        "sequence_length = 4\n",
        "\n",
        "def word_to_onehot(string):\n",
        "    one_hot = np.array([]).reshape(0,n_letters)\n",
        "    for i in string:\n",
        "      idx = char_list.index(i)\n",
        "      zero = np.zeros(shape=n_letters, dtype=int)\n",
        "      zero[idx] = 1\n",
        "      one_hot = np.vstack([one_hot, zero])\n",
        "    return one_hot\n",
        "\n",
        "def onehot_to_word(onehot_1):\n",
        "    onehot = torch.Tensor.numpy(onehot_1)\n",
        "    return char_list[onehot.argmax()]\n",
        "\n",
        "# Use RNN Packages \n",
        "class myRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layer):\n",
        "    super(myRNN,  self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layer = num_layer\n",
        "    \n",
        "    self.rnn = nn.RNN(input_size = input_size,hidden_size=hidden_size, num_layers=num_layer)\n",
        "    \n",
        "  def forward(self, x, hidden):\n",
        "    out, hidden = self.rnn(x, hidden)\n",
        "    return out, hidden\n",
        "    \n",
        "  def init_hidden(self):\n",
        "    return torch.zeros(self.num_layer, 1, self.hidden_size)\n",
        "\n",
        "\n",
        "def main():\n",
        "  n_hidden = 27\n",
        "  lr = 0.001\n",
        "  epochs = 900\n",
        "  \n",
        "  model = myRNN(n_letters, n_hidden, n_layers)\n",
        "  \n",
        "  loss_func = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 300, gamma=0.1)\n",
        "  \n",
        "  for i in range(epochs):\n",
        "    total_loss = 0\n",
        "    for j in range(n_five_words):\n",
        "      hidden = model.init_hidden()\n",
        "      string = five_words[j]\n",
        "      one_hot = torch.from_numpy(word_to_onehot(string)).type_as(torch.FloatTensor())\n",
        "      model.zero_grad()\n",
        "      hidden = model.init_hidden()\n",
        "      input = one_hot[0:-1]\n",
        "      input = torch.unsqueeze(input, 1)\n",
        "      target = np.argmax(one_hot[1:], axis=1)\n",
        "      \n",
        "      output, hidden  = model(input, hidden)\n",
        "      \n",
        "      loss = loss_func(output.squeeze(1), target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "    if i%10 == 0:\n",
        "      print('epoch%d'%i)\n",
        "      print(loss)\n",
        "      \n",
        "    scheduler.step()\n",
        "    \n",
        "  torch.save(model.state_dict(), 'trained.pth')\n",
        "  model.load_state_dict(torch.load('trained.pth'))\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    total = 0\n",
        "    positive = 0\n",
        "    total_text = 0\n",
        "    positive_text = 0\n",
        "    for i in range(n_five_words):\n",
        "      string = five_words[i]\n",
        "      one_hot = torch.from_numpy(word_to_onehot(string)).type_as(torch.FloatTensor())\n",
        "      hidden = model.init_hidden()\n",
        "      input = one_hot[0:-1]\n",
        "      input = torch.unsqueeze(input, 1)\n",
        "      target = np.argmax(one_hot[1:], axis=1)\n",
        "      output, hidden = model(input, hidden)\n",
        "      output = output.squeeze()\n",
        "      \n",
        "      output_string = string[0]\n",
        "      \n",
        "      for j in range(output.size()[0]):\n",
        "        output_string += onehot_to_word(output[j].data)\n",
        "        total_text += 1\n",
        "        \n",
        "        if string[j+1] == output_string[-1]:\n",
        "          positive_text += 1\n",
        "          \n",
        "      total += 1\n",
        "      if string[-1] == output_string[-1]:\n",
        "        positive += 1\n",
        "        \n",
        "      print('%d GT:%s OUT:%s'%(i+1, string, output_string))\n",
        "      \n",
        "    print('final text accuracy %d/%d (%.4f)'%(positive, total, positive/total))\n",
        "    print('whole text accuracy %d/%d (%.4f)' % (positive_text, total_text, positive_text / total_text))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "chars = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "char_list = [i for i in chars]\n",
        "n_letters = len(char_list)\n",
        "\n",
        "#n_layers = 1\n",
        "n_layers = 2 # LSTM\n",
        "\n",
        "five_words = ['basic','beach','below','black','brown','carry','cream','drink','error','event','exist','first','funny','guess','human','image','large','magic','mouse','night','noise','ocean','often','order','peace','phone','print','quiet','reach','rough','round','scene','score','sense','skill','sleep','small','storm','table','think','touch','twice','until','upset','voice','waste','watch','white','woman','young']\n",
        "n_five_words = len(five_words)\n",
        "\n",
        "sequence_length = 4\n",
        "\n",
        "def word_to_onehot(string):\n",
        "    one_hot = np.array([]).reshape(0,n_letters)\n",
        "    for i in string:\n",
        "      idx = char_list.index(i)\n",
        "      zero = np.zeros(shape=n_letters, dtype=int)\n",
        "      zero[idx] = 1\n",
        "      one_hot = np.vstack([one_hot, zero])\n",
        "    return one_hot\n",
        "\n",
        "def onehot_to_word(onehot_1):\n",
        "    onehot = torch.Tensor.numpy(onehot_1)\n",
        "    return char_list[onehot.argmax()]\n",
        "\n",
        "# Use LSTM Packages \n",
        "class myLSTM(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layer):\n",
        "    super(myLSTM,  self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layer = num_layer\n",
        "      \n",
        "    self.lstm = nn.LSTM(input_size = input_size,\n",
        "                        hidden_size=hidden_size,\n",
        "                        num_layers=num_layer,\n",
        "                        batch_first = True)\n",
        "    \n",
        "  def forward(self, x, h0, c0):\n",
        "    out, (hn, cn) = self.lstm(x, (h0, c0))\n",
        "    return out, (hn, cn)\n",
        "\n",
        "  def init_hidden(self):\n",
        "    return torch.zeros(self.num_layer, 1, self.hidden_size)\n",
        "  \n",
        "  def init_cell(self):\n",
        "    return torch.zeros(self.num_layer, 1, self.hidden_size)\n",
        "    \n",
        "## main function \n",
        "def main():\n",
        "  n_hidden = 26\n",
        "  lr = 0.001\n",
        "  epochs = 900\n",
        "  \n",
        "  model = myLSTM(n_letters, n_hidden, n_layers)\n",
        "\n",
        "  loss_func = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 300, gamma=0.1)\n",
        "  \n",
        "  for i in range(epochs):\n",
        "    total_loss = 0\n",
        "    for j in range(n_five_words):\n",
        "      #hidden = model.init_hidden()\n",
        "      string = five_words[j]\n",
        "      one_hot = torch.from_numpy(word_to_onehot(string)).type_as(torch.FloatTensor())\n",
        "      model.zero_grad()\n",
        "\n",
        "      h0 = model.init_hidden()\n",
        "      c0 = model.init_cell()\n",
        "\n",
        "      input = one_hot[0:-1]\n",
        "      input = torch.unsqueeze(input, 1) \n",
        "      input = input.reshape([1,4, n_letters])\n",
        "\n",
        "      target = np.argmax(one_hot[1:], axis=1)\n",
        "      output, (hc, cn) = model(input, h0, c0)\n",
        "\n",
        "      output = output.reshape([4, 1, n_hidden])     \n",
        "      loss = loss_func(output.squeeze(1), target) # LSTM\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "    if i%10 == 0:\n",
        "      print('epoch%d'%i)\n",
        "      print(loss)\n",
        "      \n",
        "    scheduler.step()\n",
        "    \n",
        "  torch.save(model.state_dict(), 'trained.pth')\n",
        "  model.load_state_dict(torch.load('trained.pth'))\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    total = 0\n",
        "    positive = 0\n",
        "    total_text = 0\n",
        "    positive_text = 0\n",
        "    for i in range(n_five_words):\n",
        "      string = five_words[i]\n",
        "      one_hot = torch.from_numpy(word_to_onehot(string)).type_as(torch.FloatTensor())\n",
        "\n",
        "      h0 = model.init_hidden()\n",
        "      c0 = model.init_cell()\n",
        "\n",
        "      input = one_hot[0:-1]\n",
        "      input = torch.unsqueeze(input, 0) # LSTM\n",
        "      input = input.reshape([1, 4, n_letters])\n",
        "\n",
        "      target = np.argmax(one_hot[1:], axis=1)\n",
        "\n",
        "      output, (hn, cn) = model(input, h0, c0)\n",
        "      output = output.reshape([4, 1, n_hidden])\n",
        "      output = output.squeeze()    \n",
        "\n",
        "      output_string = string[0]      \n",
        "      for j in range(output.size()[0]):\n",
        "        output_string += onehot_to_word(output[j].data)\n",
        "        total_text += 1\n",
        "        \n",
        "        if string[j+1] == output_string[-1]:\n",
        "          positive_text += 1\n",
        "          \n",
        "      total += 1\n",
        "      if string[-1] == output_string[-1]:\n",
        "        positive += 1\n",
        "        \n",
        "      print('%d GT:%s OUT:%s'%(i+1, string, output_string))\n",
        "      \n",
        "    print('final text accuracy %d/%d (%.4f)'%(positive, total, positive/total))\n",
        "    print('whole text accuracy %d/%d (%.4f)' % (positive_text, total_text, positive_text / total_text))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J6rcNyA7QdEV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58a584c1-3da8-43e8-f041-8a64f7d21b67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch0\n",
            "tensor(3.2696, grad_fn=<NllLossBackward0>)\n",
            "epoch10\n",
            "tensor(2.8366, grad_fn=<NllLossBackward0>)\n",
            "epoch20\n",
            "tensor(2.5785, grad_fn=<NllLossBackward0>)\n",
            "epoch30\n",
            "tensor(2.4037, grad_fn=<NllLossBackward0>)\n",
            "epoch40\n",
            "tensor(2.3369, grad_fn=<NllLossBackward0>)\n",
            "epoch50\n",
            "tensor(2.2547, grad_fn=<NllLossBackward0>)\n",
            "epoch60\n",
            "tensor(2.1960, grad_fn=<NllLossBackward0>)\n",
            "epoch70\n",
            "tensor(2.1140, grad_fn=<NllLossBackward0>)\n",
            "epoch80\n",
            "tensor(2.0453, grad_fn=<NllLossBackward0>)\n",
            "epoch90\n",
            "tensor(2.0021, grad_fn=<NllLossBackward0>)\n",
            "epoch100\n",
            "tensor(1.9730, grad_fn=<NllLossBackward0>)\n",
            "epoch110\n",
            "tensor(1.9547, grad_fn=<NllLossBackward0>)\n",
            "epoch120\n",
            "tensor(1.9416, grad_fn=<NllLossBackward0>)\n",
            "epoch130\n",
            "tensor(1.9311, grad_fn=<NllLossBackward0>)\n",
            "epoch140\n",
            "tensor(1.9173, grad_fn=<NllLossBackward0>)\n",
            "epoch150\n",
            "tensor(1.9002, grad_fn=<NllLossBackward0>)\n",
            "epoch160\n",
            "tensor(1.8766, grad_fn=<NllLossBackward0>)\n",
            "epoch170\n",
            "tensor(1.8471, grad_fn=<NllLossBackward0>)\n",
            "epoch180\n",
            "tensor(1.8303, grad_fn=<NllLossBackward0>)\n",
            "epoch190\n",
            "tensor(1.8201, grad_fn=<NllLossBackward0>)\n",
            "epoch200\n",
            "tensor(1.8147, grad_fn=<NllLossBackward0>)\n",
            "epoch210\n",
            "tensor(1.8076, grad_fn=<NllLossBackward0>)\n",
            "epoch220\n",
            "tensor(1.8001, grad_fn=<NllLossBackward0>)\n",
            "epoch230\n",
            "tensor(1.7861, grad_fn=<NllLossBackward0>)\n",
            "epoch240\n",
            "tensor(1.7776, grad_fn=<NllLossBackward0>)\n",
            "epoch250\n",
            "tensor(1.7671, grad_fn=<NllLossBackward0>)\n",
            "epoch260\n",
            "tensor(1.7615, grad_fn=<NllLossBackward0>)\n",
            "epoch270\n",
            "tensor(1.7585, grad_fn=<NllLossBackward0>)\n",
            "epoch280\n",
            "tensor(1.7570, grad_fn=<NllLossBackward0>)\n",
            "epoch290\n",
            "tensor(1.7517, grad_fn=<NllLossBackward0>)\n",
            "epoch300\n",
            "tensor(1.7440, grad_fn=<NllLossBackward0>)\n",
            "epoch310\n",
            "tensor(1.7434, grad_fn=<NllLossBackward0>)\n",
            "epoch320\n",
            "tensor(1.7427, grad_fn=<NllLossBackward0>)\n",
            "epoch330\n",
            "tensor(1.7420, grad_fn=<NllLossBackward0>)\n",
            "epoch340\n",
            "tensor(1.7412, grad_fn=<NllLossBackward0>)\n",
            "epoch350\n",
            "tensor(1.7404, grad_fn=<NllLossBackward0>)\n",
            "epoch360\n",
            "tensor(1.7395, grad_fn=<NllLossBackward0>)\n",
            "epoch370\n",
            "tensor(1.7386, grad_fn=<NllLossBackward0>)\n",
            "epoch380\n",
            "tensor(1.7377, grad_fn=<NllLossBackward0>)\n",
            "epoch390\n",
            "tensor(1.7367, grad_fn=<NllLossBackward0>)\n",
            "epoch400\n",
            "tensor(1.7357, grad_fn=<NllLossBackward0>)\n",
            "epoch410\n",
            "tensor(1.7346, grad_fn=<NllLossBackward0>)\n",
            "epoch420\n",
            "tensor(1.7336, grad_fn=<NllLossBackward0>)\n",
            "epoch430\n",
            "tensor(1.7325, grad_fn=<NllLossBackward0>)\n",
            "epoch440\n",
            "tensor(1.7315, grad_fn=<NllLossBackward0>)\n",
            "epoch450\n",
            "tensor(1.7304, grad_fn=<NllLossBackward0>)\n",
            "epoch460\n",
            "tensor(1.7294, grad_fn=<NllLossBackward0>)\n",
            "epoch470\n",
            "tensor(1.7284, grad_fn=<NllLossBackward0>)\n",
            "epoch480\n",
            "tensor(1.7275, grad_fn=<NllLossBackward0>)\n",
            "epoch490\n",
            "tensor(1.7267, grad_fn=<NllLossBackward0>)\n",
            "epoch500\n",
            "tensor(1.7258, grad_fn=<NllLossBackward0>)\n",
            "epoch510\n",
            "tensor(1.7250, grad_fn=<NllLossBackward0>)\n",
            "epoch520\n",
            "tensor(1.7243, grad_fn=<NllLossBackward0>)\n",
            "epoch530\n",
            "tensor(1.7236, grad_fn=<NllLossBackward0>)\n",
            "epoch540\n",
            "tensor(1.7230, grad_fn=<NllLossBackward0>)\n",
            "epoch550\n",
            "tensor(1.7224, grad_fn=<NllLossBackward0>)\n",
            "epoch560\n",
            "tensor(1.7218, grad_fn=<NllLossBackward0>)\n",
            "epoch570\n",
            "tensor(1.7212, grad_fn=<NllLossBackward0>)\n",
            "epoch580\n",
            "tensor(1.7206, grad_fn=<NllLossBackward0>)\n",
            "epoch590\n",
            "tensor(1.7200, grad_fn=<NllLossBackward0>)\n",
            "epoch600\n",
            "tensor(1.7192, grad_fn=<NllLossBackward0>)\n",
            "epoch610\n",
            "tensor(1.7191, grad_fn=<NllLossBackward0>)\n",
            "epoch620\n",
            "tensor(1.7191, grad_fn=<NllLossBackward0>)\n",
            "epoch630\n",
            "tensor(1.7190, grad_fn=<NllLossBackward0>)\n",
            "epoch640\n",
            "tensor(1.7189, grad_fn=<NllLossBackward0>)\n",
            "epoch650\n",
            "tensor(1.7188, grad_fn=<NllLossBackward0>)\n",
            "epoch660\n",
            "tensor(1.7187, grad_fn=<NllLossBackward0>)\n",
            "epoch670\n",
            "tensor(1.7186, grad_fn=<NllLossBackward0>)\n",
            "epoch680\n",
            "tensor(1.7185, grad_fn=<NllLossBackward0>)\n",
            "epoch690\n",
            "tensor(1.7184, grad_fn=<NllLossBackward0>)\n",
            "epoch700\n",
            "tensor(1.7183, grad_fn=<NllLossBackward0>)\n",
            "epoch710\n",
            "tensor(1.7182, grad_fn=<NllLossBackward0>)\n",
            "epoch720\n",
            "tensor(1.7181, grad_fn=<NllLossBackward0>)\n",
            "epoch730\n",
            "tensor(1.7180, grad_fn=<NllLossBackward0>)\n",
            "epoch740\n",
            "tensor(1.7179, grad_fn=<NllLossBackward0>)\n",
            "epoch750\n",
            "tensor(1.7178, grad_fn=<NllLossBackward0>)\n",
            "epoch760\n",
            "tensor(1.7177, grad_fn=<NllLossBackward0>)\n",
            "epoch770\n",
            "tensor(1.7175, grad_fn=<NllLossBackward0>)\n",
            "epoch780\n",
            "tensor(1.7174, grad_fn=<NllLossBackward0>)\n",
            "epoch790\n",
            "tensor(1.7173, grad_fn=<NllLossBackward0>)\n",
            "epoch800\n",
            "tensor(1.7172, grad_fn=<NllLossBackward0>)\n",
            "epoch810\n",
            "tensor(1.7171, grad_fn=<NllLossBackward0>)\n",
            "epoch820\n",
            "tensor(1.7169, grad_fn=<NllLossBackward0>)\n",
            "epoch830\n",
            "tensor(1.7168, grad_fn=<NllLossBackward0>)\n",
            "epoch840\n",
            "tensor(1.7167, grad_fn=<NllLossBackward0>)\n",
            "epoch850\n",
            "tensor(1.7166, grad_fn=<NllLossBackward0>)\n",
            "epoch860\n",
            "tensor(1.7164, grad_fn=<NllLossBackward0>)\n",
            "epoch870\n",
            "tensor(1.7163, grad_fn=<NllLossBackward0>)\n",
            "epoch880\n",
            "tensor(1.7162, grad_fn=<NllLossBackward0>)\n",
            "epoch890\n",
            "tensor(1.7161, grad_fn=<NllLossBackward0>)\n",
            "1 GT:basic OUT:besic\n",
            "2 GT:beach OUT:beach\n",
            "3 GT:below OUT:beaoe\n",
            "4 GT:black OUT:beack\n",
            "5 GT:brown OUT:beonn\n",
            "6 GT:carry OUT:crrry\n",
            "7 GT:cream OUT:creae\n",
            "8 GT:drink OUT:drink\n",
            "9 GT:error OUT:evror\n",
            "10 GT:event OUT:event\n",
            "11 GT:exist OUT:evist\n",
            "12 GT:first OUT:first\n",
            "13 GT:funny OUT:finny\n",
            "14 GT:guess OUT:gueee\n",
            "15 GT:human OUT:human\n",
            "16 GT:image OUT:iaage\n",
            "17 GT:large OUT:large\n",
            "18 GT:magic OUT:magic\n",
            "19 GT:mouse OUT:mause\n",
            "20 GT:night OUT:nighe\n",
            "21 GT:noise OUT:niise\n",
            "22 GT:ocean OUT:ofean\n",
            "23 GT:often OUT:often\n",
            "24 GT:order OUT:ofdee\n",
            "25 GT:peace OUT:prace\n",
            "26 GT:phone OUT:prine\n",
            "27 GT:print OUT:pront\n",
            "28 GT:quiet OUT:quiee\n",
            "29 GT:reach OUT:roach\n",
            "30 GT:rough OUT:rounh\n",
            "31 GT:round OUT:round\n",
            "32 GT:scene OUT:scene\n",
            "33 GT:score OUT:scere\n",
            "34 GT:sense OUT:scnse\n",
            "35 GT:skill OUT:scill\n",
            "36 GT:sleep OUT:sceee\n",
            "37 GT:small OUT:sclll\n",
            "38 GT:storm OUT:score\n",
            "39 GT:table OUT:twble\n",
            "40 GT:think OUT:twink\n",
            "41 GT:touch OUT:twuch\n",
            "42 GT:twice OUT:twice\n",
            "43 GT:until OUT:upiil\n",
            "44 GT:upset OUT:upsee\n",
            "45 GT:voice OUT:voice\n",
            "46 GT:waste OUT:watte\n",
            "47 GT:watch OUT:watte\n",
            "48 GT:white OUT:waite\n",
            "49 GT:woman OUT:waman\n",
            "50 GT:young OUT:young\n",
            "final text accuracy 40/50 (0.8000)\n",
            "whole text accuracy 153/200 (0.7650)\n"
          ]
        }
      ]
    }
  ]
}