{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/champsleague/DeepLearning/blob/main/Lab08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPuUBNJtQJYv",
        "outputId": "60a2bb76-017c-4885-8492-b887807368b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch0\n",
            "tensor(3.2802, grad_fn=<NllLossBackward0>)\n",
            "epoch10\n",
            "tensor(2.5830, grad_fn=<NllLossBackward0>)\n",
            "epoch20\n",
            "tensor(2.4084, grad_fn=<NllLossBackward0>)\n",
            "epoch30\n",
            "tensor(2.1654, grad_fn=<NllLossBackward0>)\n",
            "epoch40\n",
            "tensor(2.0141, grad_fn=<NllLossBackward0>)\n",
            "epoch50\n",
            "tensor(1.9203, grad_fn=<NllLossBackward0>)\n",
            "epoch60\n",
            "tensor(1.8524, grad_fn=<NllLossBackward0>)\n",
            "epoch70\n",
            "tensor(1.7906, grad_fn=<NllLossBackward0>)\n",
            "epoch80\n",
            "tensor(1.7311, grad_fn=<NllLossBackward0>)\n",
            "epoch90\n",
            "tensor(1.6833, grad_fn=<NllLossBackward0>)\n",
            "epoch100\n",
            "tensor(1.6532, grad_fn=<NllLossBackward0>)\n",
            "epoch110\n",
            "tensor(1.6163, grad_fn=<NllLossBackward0>)\n",
            "epoch120\n",
            "tensor(1.5561, grad_fn=<NllLossBackward0>)\n",
            "epoch130\n",
            "tensor(1.5378, grad_fn=<NllLossBackward0>)\n",
            "epoch140\n",
            "tensor(1.5297, grad_fn=<NllLossBackward0>)\n",
            "epoch150\n",
            "tensor(1.5243, grad_fn=<NllLossBackward0>)\n",
            "epoch160\n",
            "tensor(1.5202, grad_fn=<NllLossBackward0>)\n",
            "epoch170\n",
            "tensor(1.5177, grad_fn=<NllLossBackward0>)\n",
            "epoch180\n",
            "tensor(1.5156, grad_fn=<NllLossBackward0>)\n",
            "epoch190\n",
            "tensor(1.5140, grad_fn=<NllLossBackward0>)\n",
            "epoch200\n",
            "tensor(1.5133, grad_fn=<NllLossBackward0>)\n",
            "epoch210\n",
            "tensor(1.5121, grad_fn=<NllLossBackward0>)\n",
            "epoch220\n",
            "tensor(1.5113, grad_fn=<NllLossBackward0>)\n",
            "epoch230\n",
            "tensor(1.5107, grad_fn=<NllLossBackward0>)\n",
            "epoch240\n",
            "tensor(1.5133, grad_fn=<NllLossBackward0>)\n",
            "epoch250\n",
            "tensor(1.5106, grad_fn=<NllLossBackward0>)\n",
            "epoch260\n",
            "tensor(1.5098, grad_fn=<NllLossBackward0>)\n",
            "epoch270\n",
            "tensor(1.5095, grad_fn=<NllLossBackward0>)\n",
            "epoch280\n",
            "tensor(1.5094, grad_fn=<NllLossBackward0>)\n",
            "epoch290\n",
            "tensor(1.5091, grad_fn=<NllLossBackward0>)\n",
            "epoch300\n",
            "tensor(1.5090, grad_fn=<NllLossBackward0>)\n",
            "epoch310\n",
            "tensor(1.5090, grad_fn=<NllLossBackward0>)\n",
            "epoch320\n",
            "tensor(1.5089, grad_fn=<NllLossBackward0>)\n",
            "epoch330\n",
            "tensor(1.5089, grad_fn=<NllLossBackward0>)\n",
            "epoch340\n",
            "tensor(1.5089, grad_fn=<NllLossBackward0>)\n",
            "epoch350\n",
            "tensor(1.5089, grad_fn=<NllLossBackward0>)\n",
            "epoch360\n",
            "tensor(1.5089, grad_fn=<NllLossBackward0>)\n",
            "epoch370\n",
            "tensor(1.5088, grad_fn=<NllLossBackward0>)\n",
            "epoch380\n",
            "tensor(1.5088, grad_fn=<NllLossBackward0>)\n",
            "epoch390\n",
            "tensor(1.5088, grad_fn=<NllLossBackward0>)\n",
            "epoch400\n",
            "tensor(1.5088, grad_fn=<NllLossBackward0>)\n",
            "epoch410\n",
            "tensor(1.5087, grad_fn=<NllLossBackward0>)\n",
            "epoch420\n",
            "tensor(1.5087, grad_fn=<NllLossBackward0>)\n",
            "epoch430\n",
            "tensor(1.5087, grad_fn=<NllLossBackward0>)\n",
            "epoch440\n",
            "tensor(1.5087, grad_fn=<NllLossBackward0>)\n",
            "epoch450\n",
            "tensor(1.5087, grad_fn=<NllLossBackward0>)\n",
            "epoch460\n",
            "tensor(1.5086, grad_fn=<NllLossBackward0>)\n",
            "epoch470\n",
            "tensor(1.5086, grad_fn=<NllLossBackward0>)\n",
            "epoch480\n",
            "tensor(1.5086, grad_fn=<NllLossBackward0>)\n",
            "epoch490\n",
            "tensor(1.5086, grad_fn=<NllLossBackward0>)\n",
            "epoch500\n",
            "tensor(1.5086, grad_fn=<NllLossBackward0>)\n",
            "epoch510\n",
            "tensor(1.5085, grad_fn=<NllLossBackward0>)\n",
            "epoch520\n",
            "tensor(1.5085, grad_fn=<NllLossBackward0>)\n",
            "epoch530\n",
            "tensor(1.5085, grad_fn=<NllLossBackward0>)\n",
            "epoch540\n",
            "tensor(1.5085, grad_fn=<NllLossBackward0>)\n",
            "epoch550\n",
            "tensor(1.5085, grad_fn=<NllLossBackward0>)\n",
            "epoch560\n",
            "tensor(1.5085, grad_fn=<NllLossBackward0>)\n",
            "epoch570\n",
            "tensor(1.5085, grad_fn=<NllLossBackward0>)\n",
            "epoch580\n",
            "tensor(1.5085, grad_fn=<NllLossBackward0>)\n",
            "epoch590\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch600\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch610\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch620\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch630\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch640\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch650\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch660\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch670\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch680\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch690\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch700\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch710\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch720\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch730\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch740\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch750\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch760\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch770\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch780\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch790\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch800\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch810\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch820\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch830\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch840\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch850\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch860\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch870\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch880\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "epoch890\n",
            "tensor(1.5084, grad_fn=<NllLossBackward0>)\n",
            "1 GT:basic OUT:besic\n",
            "2 GT:beach OUT:beach\n",
            "3 GT:below OUT:beaoh\n",
            "4 GT:black OUT:beack\n",
            "5 GT:brown OUT:beoun\n",
            "6 GT:carry OUT:crrry\n",
            "7 GT:cream OUT:cream\n",
            "8 GT:drink OUT:drink\n",
            "9 GT:error OUT:exeor\n",
            "10 GT:event OUT:exent\n",
            "11 GT:exist OUT:exist\n",
            "12 GT:first OUT:furst\n",
            "13 GT:funny OUT:funsy\n",
            "14 GT:guess OUT:guest\n",
            "15 GT:human OUT:honan\n",
            "16 GT:image OUT:image\n",
            "17 GT:large OUT:large\n",
            "18 GT:magic OUT:magic\n",
            "19 GT:mouse OUT:mause\n",
            "20 GT:night OUT:noiht\n",
            "21 GT:noise OUT:noise\n",
            "22 GT:ocean OUT:orean\n",
            "23 GT:often OUT:orten\n",
            "24 GT:order OUT:oreer\n",
            "25 GT:peace OUT:prace\n",
            "26 GT:phone OUT:prone\n",
            "27 GT:print OUT:print\n",
            "28 GT:quiet OUT:quiet\n",
            "29 GT:reach OUT:roach\n",
            "30 GT:rough OUT:rounh\n",
            "31 GT:round OUT:round\n",
            "32 GT:scene OUT:scene\n",
            "33 GT:score OUT:scere\n",
            "34 GT:sense OUT:scnee\n",
            "35 GT:skill OUT:scill\n",
            "36 GT:sleep OUT:sceet\n",
            "37 GT:small OUT:scoll\n",
            "38 GT:storm OUT:scorm\n",
            "39 GT:table OUT:table\n",
            "40 GT:think OUT:taink\n",
            "41 GT:touch OUT:tauch\n",
            "42 GT:twice OUT:taice\n",
            "43 GT:until OUT:upsil\n",
            "44 GT:upset OUT:upset\n",
            "45 GT:voice OUT:voice\n",
            "46 GT:waste OUT:wasce\n",
            "47 GT:watch OUT:wasch\n",
            "48 GT:white OUT:waite\n",
            "49 GT:woman OUT:waman\n",
            "50 GT:young OUT:young\n",
            "final text accuracy 47/50 (0.9400)\n",
            "whole text accuracy 157/200 (0.7850)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "chars = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "char_list = [i for i in chars]\n",
        "n_letters = len(char_list)\n",
        "\n",
        "n_layers = 2\n",
        "\n",
        "five_words = ['basic','beach','below','black','brown','carry','cream','drink','error','event','exist','first','funny','guess','human','image','large','magic','mouse','night','noise','ocean','often','order','peace','phone','print','quiet','reach','rough','round','scene','score','sense','skill','sleep','small','storm','table','think','touch','twice','until','upset','voice','waste','watch','white','woman','young']\n",
        "n_five_words = len(five_words)\n",
        "\n",
        "sequence_length = 4\n",
        "\n",
        "def word_to_onehot(string):\n",
        "    one_hot = np.array([]).reshape(0,n_letters)\n",
        "    for i in string:\n",
        "      idx = char_list.index(i)\n",
        "      zero = np.zeros(shape=n_letters, dtype=int)\n",
        "      zero[idx] = 1\n",
        "      one_hot = np.vstack([one_hot, zero])\n",
        "    return one_hot\n",
        "\n",
        "def onehot_to_word(onehot_1):\n",
        "    onehot = torch.Tensor.numpy(onehot_1)\n",
        "    return char_list[onehot.argmax()]\n",
        "\n",
        "# Use RNN Packages \n",
        "class myRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layer):\n",
        "    super(myRNN,  self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layer = num_layer\n",
        "    \n",
        "    self.rnn = nn.RNN(input_size = input_size,hidden_size=hidden_size, num_layers=num_layer)\n",
        "    \n",
        "  def forward(self, x, hidden):\n",
        "    out, hidden = self.rnn(x, hidden)\n",
        "    return out, hidden\n",
        "    \n",
        "  def init_hidden(self):\n",
        "    return torch.zeros(self.num_layer, 1, self.hidden_size)\n",
        "\n",
        "\n",
        "def main():\n",
        "  n_hidden = 27\n",
        "  lr = 0.001\n",
        "  epochs = 900\n",
        "  \n",
        "  model = myRNN(n_letters, n_hidden, n_layers)\n",
        "  \n",
        "  loss_func = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 300, gamma=0.1)\n",
        "  \n",
        "  for i in range(epochs):\n",
        "    total_loss = 0\n",
        "    for j in range(n_five_words):\n",
        "      hidden = model.init_hidden()\n",
        "      string = five_words[j]\n",
        "      one_hot = torch.from_numpy(word_to_onehot(string)).type_as(torch.FloatTensor())\n",
        "      model.zero_grad()\n",
        "      hidden = model.init_hidden()\n",
        "      input = one_hot[0:-1]\n",
        "      input = torch.unsqueeze(input, 1)\n",
        "      target = np.argmax(one_hot[1:], axis=1)\n",
        "      \n",
        "      output, hidden  = model(input, hidden)\n",
        "      \n",
        "      loss = loss_func(output.squeeze(1), target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "    if i%10 == 0:\n",
        "      print('epoch%d'%i)\n",
        "      print(loss)\n",
        "      \n",
        "    scheduler.step()\n",
        "    \n",
        "  torch.save(model.state_dict(), 'trained.pth')\n",
        "  model.load_state_dict(torch.load('trained.pth'))\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    total = 0\n",
        "    positive = 0\n",
        "    total_text = 0\n",
        "    positive_text = 0\n",
        "    for i in range(n_five_words):\n",
        "      string = five_words[i]\n",
        "      one_hot = torch.from_numpy(word_to_onehot(string)).type_as(torch.FloatTensor())\n",
        "      hidden = model.init_hidden()\n",
        "      input = one_hot[0:-1]\n",
        "      input = torch.unsqueeze(input, 1)\n",
        "      target = np.argmax(one_hot[1:], axis=1)\n",
        "      output, hidden = model(input, hidden)\n",
        "      output = output.squeeze()\n",
        "      \n",
        "      output_string = string[0]\n",
        "      \n",
        "      for j in range(output.size()[0]):\n",
        "        output_string += onehot_to_word(output[j].data)\n",
        "        total_text += 1\n",
        "        \n",
        "        if string[j+1] == output_string[-1]:\n",
        "          positive_text += 1\n",
        "          \n",
        "      total += 1\n",
        "      if string[-1] == output_string[-1]:\n",
        "        positive += 1\n",
        "        \n",
        "      print('%d GT:%s OUT:%s'%(i+1, string, output_string))\n",
        "      \n",
        "    print('final text accuracy %d/%d (%.4f)'%(positive, total, positive/total))\n",
        "    print('whole text accuracy %d/%d (%.4f)' % (positive_text, total_text, positive_text / total_text))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "chars = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "char_list = [i for i in chars]\n",
        "n_letters = len(char_list)\n",
        "\n",
        "n_layers = 1\n",
        "\n",
        "five_words = ['basic','beach','below','black','brown','carry','cream','drink','error','event','exist','first','funny','guess','human','image','large','magic','mouse','night','noise','ocean','often','order','peace','phone','print','quiet','reach','rough','round','scene','score','sense','skill','sleep','small','storm','table','think','touch','twice','until','upset','voice','waste','watch','white','woman','young']\n",
        "n_five_words = len(five_words)\n",
        "\n",
        "sequence_length = 4\n",
        "\n",
        "def word_to_onehot(string):\n",
        "    one_hot = np.array([]).reshape(0,n_letters)\n",
        "    for i in string:\n",
        "      idx = char_list.index(i)\n",
        "      zero = np.zeros(shape=n_letters, dtype=int)\n",
        "      zero[idx] = 1\n",
        "      one_hot = np.vstack([one_hot, zero])\n",
        "    return one_hot\n",
        "\n",
        "def onehot_to_word(onehot_1):\n",
        "    onehot = torch.Tensor.numpy(onehot_1)\n",
        "    return char_list[onehot.argmax()]\n",
        "\n",
        "\n",
        "\n",
        "class myRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers):\n",
        "        super(myRNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        out, hidden = self.lstm(x, hidden)\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size=4):\n",
        "        return (\n",
        "            torch.zeros(self.num_layers, batch_size, self.hidden_size),\n",
        "            torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "  n_hidden = 26\n",
        "  lr = 0.001\n",
        "  epochs = 900\n",
        "  \n",
        "  model = myRNN(n_letters, n_hidden, n_layers)\n",
        "  \n",
        "  loss_func = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 900, gamma=0.1)\n",
        "  \n",
        "  for i in range(epochs):\n",
        "    total_loss = 0\n",
        "    for j in range(n_five_words):\n",
        "      hidden = model.init_hidden()\n",
        "      string = five_words[j]\n",
        "      one_hot = torch.from_numpy(word_to_onehot(string)).type_as(torch.FloatTensor())\n",
        "      model.zero_grad()\n",
        "      hidden = model.init_hidden()\n",
        "      input = one_hot[0:-1]\n",
        "      input = torch.unsqueeze(input, 1)\n",
        "      target = np.argmax(one_hot[1:], axis=1)\n",
        "      \n",
        "      output, hidden  = model(input, hidden)\n",
        "      \n",
        "      loss = loss_func(output.squeeze(1), target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "    if i%10 == 0:\n",
        "      print('epoch%d'%i)\n",
        "      print(loss)\n",
        "      \n",
        "    scheduler.step()\n",
        "    \n",
        "  torch.save(model.state_dict(), 'trained.pth')\n",
        "  model.load_state_dict(torch.load('trained.pth'))\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    total = 0\n",
        "    positive = 0\n",
        "    total_text = 0\n",
        "    positive_text = 0\n",
        "    for i in range(n_five_words):\n",
        "      string = five_words[i]\n",
        "      one_hot = torch.from_numpy(word_to_onehot(string)).type_as(torch.FloatTensor())\n",
        "      hidden = model.init_hidden()\n",
        "      input = one_hot[0:-1]\n",
        "      input = torch.unsqueeze(input, 1)\n",
        "      target = np.argmax(one_hot[1:], axis=1)\n",
        "      output, hidden = model(input, hidden)\n",
        "      output = output.squeeze()\n",
        "      \n",
        "      output_string = string[0]\n",
        "      \n",
        "      for j in range(output.size()[0]):\n",
        "        output_string += onehot_to_word(output[j].data)\n",
        "        total_text += 1\n",
        "        \n",
        "        if string[j+1] == output_string[-1]:\n",
        "          positive_text += 1\n",
        "          \n",
        "      total += 1\n",
        "      if string[-1] == output_string[-1]:\n",
        "        positive += 1\n",
        "        \n",
        "      print('%d GT:%s OUT:%s'%(i+1, string, output_string))\n",
        "      \n",
        "    print('final text accuracy %d/%d (%.4f)'%(positive, total, positive/total))\n",
        "    print('whole text accuracy %d/%d (%.4f)' % (positive_text, total_text, positive_text / total_text))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J6rcNyA7QdEV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a281932-1e77-48e1-8f65-15f6f5ae1788"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch0\n",
            "tensor(3.2429, grad_fn=<NllLossBackward0>)\n",
            "epoch10\n",
            "tensor(3.0528, grad_fn=<NllLossBackward0>)\n",
            "epoch20\n",
            "tensor(2.8729, grad_fn=<NllLossBackward0>)\n",
            "epoch30\n",
            "tensor(2.7400, grad_fn=<NllLossBackward0>)\n",
            "epoch40\n",
            "tensor(2.6515, grad_fn=<NllLossBackward0>)\n",
            "epoch50\n",
            "tensor(2.5891, grad_fn=<NllLossBackward0>)\n",
            "epoch60\n",
            "tensor(2.5436, grad_fn=<NllLossBackward0>)\n",
            "epoch70\n",
            "tensor(2.5097, grad_fn=<NllLossBackward0>)\n",
            "epoch80\n",
            "tensor(2.4835, grad_fn=<NllLossBackward0>)\n",
            "epoch90\n",
            "tensor(2.4623, grad_fn=<NllLossBackward0>)\n",
            "epoch100\n",
            "tensor(2.4448, grad_fn=<NllLossBackward0>)\n",
            "epoch110\n",
            "tensor(2.4300, grad_fn=<NllLossBackward0>)\n",
            "epoch120\n",
            "tensor(2.4173, grad_fn=<NllLossBackward0>)\n",
            "epoch130\n",
            "tensor(2.4063, grad_fn=<NllLossBackward0>)\n",
            "epoch140\n",
            "tensor(2.3968, grad_fn=<NllLossBackward0>)\n",
            "epoch150\n",
            "tensor(2.3885, grad_fn=<NllLossBackward0>)\n",
            "epoch160\n",
            "tensor(2.3813, grad_fn=<NllLossBackward0>)\n",
            "epoch170\n",
            "tensor(2.3750, grad_fn=<NllLossBackward0>)\n",
            "epoch180\n",
            "tensor(2.3697, grad_fn=<NllLossBackward0>)\n",
            "epoch190\n",
            "tensor(2.3651, grad_fn=<NllLossBackward0>)\n",
            "epoch200\n",
            "tensor(2.3612, grad_fn=<NllLossBackward0>)\n",
            "epoch210\n",
            "tensor(2.3578, grad_fn=<NllLossBackward0>)\n",
            "epoch220\n",
            "tensor(2.3549, grad_fn=<NllLossBackward0>)\n",
            "epoch230\n",
            "tensor(2.3523, grad_fn=<NllLossBackward0>)\n",
            "epoch240\n",
            "tensor(2.3500, grad_fn=<NllLossBackward0>)\n",
            "epoch250\n",
            "tensor(2.3479, grad_fn=<NllLossBackward0>)\n",
            "epoch260\n",
            "tensor(2.3461, grad_fn=<NllLossBackward0>)\n",
            "epoch270\n",
            "tensor(2.3445, grad_fn=<NllLossBackward0>)\n",
            "epoch280\n",
            "tensor(2.3430, grad_fn=<NllLossBackward0>)\n",
            "epoch290\n",
            "tensor(2.3417, grad_fn=<NllLossBackward0>)\n",
            "epoch300\n",
            "tensor(2.3406, grad_fn=<NllLossBackward0>)\n",
            "epoch310\n",
            "tensor(2.3396, grad_fn=<NllLossBackward0>)\n",
            "epoch320\n",
            "tensor(2.3387, grad_fn=<NllLossBackward0>)\n",
            "epoch330\n",
            "tensor(2.3378, grad_fn=<NllLossBackward0>)\n",
            "epoch340\n",
            "tensor(2.3371, grad_fn=<NllLossBackward0>)\n",
            "epoch350\n",
            "tensor(2.3365, grad_fn=<NllLossBackward0>)\n",
            "epoch360\n",
            "tensor(2.3359, grad_fn=<NllLossBackward0>)\n",
            "epoch370\n",
            "tensor(2.3353, grad_fn=<NllLossBackward0>)\n",
            "epoch380\n",
            "tensor(2.3348, grad_fn=<NllLossBackward0>)\n",
            "epoch390\n",
            "tensor(2.3344, grad_fn=<NllLossBackward0>)\n",
            "epoch400\n",
            "tensor(2.3340, grad_fn=<NllLossBackward0>)\n",
            "epoch410\n",
            "tensor(2.3336, grad_fn=<NllLossBackward0>)\n",
            "epoch420\n",
            "tensor(2.3333, grad_fn=<NllLossBackward0>)\n",
            "epoch430\n",
            "tensor(2.3330, grad_fn=<NllLossBackward0>)\n",
            "epoch440\n",
            "tensor(2.3327, grad_fn=<NllLossBackward0>)\n",
            "epoch450\n",
            "tensor(2.3325, grad_fn=<NllLossBackward0>)\n",
            "epoch460\n",
            "tensor(2.3323, grad_fn=<NllLossBackward0>)\n",
            "epoch470\n",
            "tensor(2.3321, grad_fn=<NllLossBackward0>)\n",
            "epoch480\n",
            "tensor(2.3319, grad_fn=<NllLossBackward0>)\n",
            "epoch490\n",
            "tensor(2.3317, grad_fn=<NllLossBackward0>)\n",
            "epoch500\n",
            "tensor(2.3316, grad_fn=<NllLossBackward0>)\n",
            "epoch510\n",
            "tensor(2.3314, grad_fn=<NllLossBackward0>)\n",
            "epoch520\n",
            "tensor(2.3313, grad_fn=<NllLossBackward0>)\n",
            "epoch530\n",
            "tensor(2.3312, grad_fn=<NllLossBackward0>)\n",
            "epoch540\n",
            "tensor(2.3311, grad_fn=<NllLossBackward0>)\n",
            "epoch550\n",
            "tensor(2.3310, grad_fn=<NllLossBackward0>)\n",
            "epoch560\n",
            "tensor(2.3309, grad_fn=<NllLossBackward0>)\n",
            "epoch570\n",
            "tensor(2.3309, grad_fn=<NllLossBackward0>)\n",
            "epoch580\n",
            "tensor(2.3308, grad_fn=<NllLossBackward0>)\n",
            "epoch590\n",
            "tensor(2.3307, grad_fn=<NllLossBackward0>)\n",
            "epoch600\n",
            "tensor(2.3307, grad_fn=<NllLossBackward0>)\n",
            "epoch610\n",
            "tensor(2.3306, grad_fn=<NllLossBackward0>)\n",
            "epoch620\n",
            "tensor(2.3306, grad_fn=<NllLossBackward0>)\n",
            "epoch630\n",
            "tensor(2.3306, grad_fn=<NllLossBackward0>)\n",
            "epoch640\n",
            "tensor(2.3305, grad_fn=<NllLossBackward0>)\n",
            "epoch650\n",
            "tensor(2.3305, grad_fn=<NllLossBackward0>)\n",
            "epoch660\n",
            "tensor(2.3305, grad_fn=<NllLossBackward0>)\n",
            "epoch670\n",
            "tensor(2.3304, grad_fn=<NllLossBackward0>)\n",
            "epoch680\n",
            "tensor(2.3304, grad_fn=<NllLossBackward0>)\n",
            "epoch690\n",
            "tensor(2.3304, grad_fn=<NllLossBackward0>)\n",
            "epoch700\n",
            "tensor(2.3304, grad_fn=<NllLossBackward0>)\n",
            "epoch710\n",
            "tensor(2.3304, grad_fn=<NllLossBackward0>)\n",
            "epoch720\n",
            "tensor(2.3303, grad_fn=<NllLossBackward0>)\n",
            "epoch730\n",
            "tensor(2.3303, grad_fn=<NllLossBackward0>)\n",
            "epoch740\n",
            "tensor(2.3303, grad_fn=<NllLossBackward0>)\n",
            "epoch750\n",
            "tensor(2.3303, grad_fn=<NllLossBackward0>)\n",
            "epoch760\n",
            "tensor(2.3303, grad_fn=<NllLossBackward0>)\n",
            "epoch770\n",
            "tensor(2.3303, grad_fn=<NllLossBackward0>)\n",
            "epoch780\n",
            "tensor(2.3303, grad_fn=<NllLossBackward0>)\n",
            "epoch790\n",
            "tensor(2.3303, grad_fn=<NllLossBackward0>)\n",
            "epoch800\n",
            "tensor(2.3303, grad_fn=<NllLossBackward0>)\n",
            "epoch810\n",
            "tensor(2.3303, grad_fn=<NllLossBackward0>)\n",
            "epoch820\n",
            "tensor(2.3303, grad_fn=<NllLossBackward0>)\n",
            "epoch830\n",
            "tensor(2.3303, grad_fn=<NllLossBackward0>)\n",
            "epoch840\n",
            "tensor(2.3303, grad_fn=<NllLossBackward0>)\n",
            "epoch850\n",
            "tensor(2.3303, grad_fn=<NllLossBackward0>)\n",
            "epoch860\n",
            "tensor(2.3303, grad_fn=<NllLossBackward0>)\n",
            "epoch870\n",
            "tensor(2.3303, grad_fn=<NllLossBackward0>)\n",
            "epoch880\n",
            "tensor(2.3303, grad_fn=<NllLossBackward0>)\n",
            "epoch890\n",
            "tensor(2.3303, grad_fn=<NllLossBackward0>)\n",
            "1 GT:basic OUT:becec\n",
            "2 GT:beach OUT:beace\n",
            "3 GT:below OUT:beaeu\n",
            "4 GT:black OUT:beece\n",
            "5 GT:brown OUT:beoua\n",
            "6 GT:carry OUT:cecoo\n",
            "7 GT:cream OUT:ceoac\n",
            "8 GT:drink OUT:deoct\n",
            "9 GT:error OUT:eaoou\n",
            "10 GT:event OUT:eaeat\n",
            "11 GT:exist OUT:eaice\n",
            "12 GT:first OUT:fucoe\n",
            "13 GT:funny OUT:funtt\n",
            "14 GT:guess OUT:ghnae\n",
            "15 GT:human OUT:hinac\n",
            "16 GT:image OUT:icach\n",
            "17 GT:large OUT:lecoh\n",
            "18 GT:magic OUT:machc\n",
            "19 GT:mouse OUT:maune\n",
            "20 GT:night OUT:ntchi\n",
            "21 GT:noise OUT:ntuce\n",
            "22 GT:ocean OUT:oueac\n",
            "23 GT:often OUT:ouuea\n",
            "24 GT:order OUT:ouoea\n",
            "25 GT:peace OUT:phace\n",
            "26 GT:phone OUT:phiut\n",
            "27 GT:print OUT:phoct\n",
            "28 GT:quiet OUT:qunca\n",
            "29 GT:reach OUT:roace\n",
            "30 GT:rough OUT:rounh\n",
            "31 GT:round OUT:rount\n",
            "32 GT:scene OUT:seeat\n",
            "33 GT:score OUT:seeuo\n",
            "34 GT:sense OUT:seate\n",
            "35 GT:skill OUT:seice\n",
            "36 GT:sleep OUT:seeaa\n",
            "37 GT:small OUT:seace\n",
            "38 GT:storm OUT:seeuo\n",
            "39 GT:table OUT:tecee\n",
            "40 GT:think OUT:teict\n",
            "41 GT:touch OUT:teune\n",
            "42 GT:twice OUT:teace\n",
            "43 GT:until OUT:untec\n",
            "44 GT:upset OUT:unhea\n",
            "45 GT:voice OUT:veuce\n",
            "46 GT:waste OUT:wacee\n",
            "47 GT:watch OUT:wacee\n",
            "48 GT:white OUT:waice\n",
            "49 GT:woman OUT:wauac\n",
            "50 GT:young OUT:yount\n",
            "final text accuracy 14/50 (0.2800)\n",
            "whole text accuracy 65/200 (0.3250)\n"
          ]
        }
      ]
    }
  ]
}
