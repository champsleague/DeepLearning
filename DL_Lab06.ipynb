{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/champsleague/DeepLearning/blob/main/DL_Lab06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3T89ahgGEs4j",
        "outputId": "007b1fac-946e-45ea-952c-c955c9c9c3c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Dependencies"
      ],
      "metadata": {
        "id": "hqQcbFXKiQFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n"
      ],
      "metadata": {
        "id": "nAa70ucnFqxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Device Definition "
      ],
      "metadata": {
        "id": "iVhjkULhnYcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available() \n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "TwYcwLl3ncfz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df45ee32-3cb9-48e0-b906-a533387368a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Hyperparameters"
      ],
      "metadata": {
        "id": "P5e6ul2giXgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "print_train_step = int(1000*16/batch_size)\n",
        "#learnning_rate = 0.01\n",
        "learnning_rate = 0.02\n",
        "epochs = 10\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
      ],
      "metadata": {
        "id": "YNxnYsdGiVCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading & Loading MNIST data"
      ],
      "metadata": {
        "id": "l1HdTz8SjLGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.MNIST(\n",
        "    './data', train=True, download=True,\n",
        "    transform=transforms.Compose([\n",
        "                                  transforms.Resize([112, 112]),\n",
        "                                  transforms.ToTensor(),\n",
        "                                  transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                  ])\n",
        "    )\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "test_dataset = datasets.MNIST(\n",
        "    './data', train=False, \n",
        "    transform=transforms.Compose([\n",
        "                                  transforms.Resize([112, 112]),\n",
        "                                  transforms.ToTensor(),\n",
        "                                  transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                  ])\n",
        "    )\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "print('number of training data : ',len(train_dataset))\n",
        "print('number of test data : ',len(test_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdID9q6yjKs7",
        "outputId": "5de8ae7a-7a42-4502-c8ca-833704d9a952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 145916551.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 38664441.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 34001432.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 18695317.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "number of training data :  60000\n",
            "number of test data :  10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define model class (Transfer Learning)"
      ],
      "metadata": {
        "id": "DucjpyqijqZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet18(pretrained=True)\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhknPC7OjhZZ",
        "outputId": "4d95a882-c819-4084-e3f6-f641576e10e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 273MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning\n"
      ],
      "metadata": {
        "id": "KjFHfQNFFUrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.conv1 = nn.Conv2d(1, 64, kernel_size=(3, 3), stride = (1, 1), padding = (1, 1), bias=False)\n",
        "model.fc = nn.Linear(512, 10, bias = True)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "model.conv1.weight.requires_grad = True\n",
        "model.bn1.weight.requires_grad = True\n",
        "model.bn1.bias.requires_grad = True\n",
        "model.fc.weight.requires_grad = True\n",
        "model.fc.bias.requires_grad = True\n",
        "\n",
        "# params_to_update = []\n",
        "# for name, param in model.named_parameters():\n",
        "#   if param.requires_grad == True:\n",
        "#     print(name)\n",
        "#     params_to_update.append(param)  \n",
        "\n",
        "# print(model)\n",
        "\n",
        "params_to_update = []\n",
        "for name, param in model.named_parameters():\n",
        "  if 'conv1' in name:\n",
        "    print(name)    \n",
        "    param.requires_grad = True\n",
        "    params_to_update.append(param)\n",
        "  elif 'fc' in name:\n",
        "    print(name)    \n",
        "    param.requires_grad = True\n",
        "    params_to_update.append(param)\n",
        "  else:\n",
        "    param.requires_grad = False\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeVazHpPFgBg",
        "outputId": "6035b9f2-8697-43f1-98e7-a77dd575a717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight\n",
            "layer1.0.conv1.weight\n",
            "layer1.1.conv1.weight\n",
            "layer2.0.conv1.weight\n",
            "layer2.1.conv1.weight\n",
            "layer3.0.conv1.weight\n",
            "layer3.1.conv1.weight\n",
            "layer4.0.conv1.weight\n",
            "layer4.1.conv1.weight\n",
            "fc.weight\n",
            "fc.bias\n",
            "ResNet(\n",
            "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Loss Function & Optimizer"
      ],
      "metadata": {
        "id": "h8PKvFPxu73G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adadelta(params_to_update, lr=learnning_rate)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
        "loss_function = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "Kxh7gIRWvCBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Model Training"
      ],
      "metadata": {
        "id": "rxM4DzoNuy05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, loss_function, epoch):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = loss_function(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "        \n",
        "    if batch_idx % print_train_step == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader), loss.item()))\n"
      ],
      "metadata": {
        "id": "lTkSBYrStLF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Model Evaluation"
      ],
      "metadata": {
        "id": "6ldWI23e0JO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, test_loader, loss_function):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      output = model(data)\n",
        "      test_loss += loss_function(output, target) # sum up batch loss\n",
        "      pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "      \n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "      test_loss, correct, len(test_loader.dataset),\n",
        "      100. * correct / len(test_loader.dataset)))\n"
      ],
      "metadata": {
        "id": "Ud_28GavyYKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Model\n"
      ],
      "metadata": {
        "id": "fK6xWqCG6DPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  train(model, device, train_loader, optimizer, loss_function, epoch)\n",
        "  test(model, device, test_loader, loss_function)\n",
        "  scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOztoOpF5nPr",
        "outputId": "a46ed299-4ea1-4fe9-ec24-e1a729a5f7b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.619268\n",
            "Train Epoch: 0 [16000/60000 (27%)]\tLoss: 0.361363\n",
            "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.123983\n",
            "Train Epoch: 0 [48000/60000 (80%)]\tLoss: 0.062665\n",
            "\n",
            "Test set: Average loss: 0.0006, Accuracy: 9873/10000 (99%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.075639\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.084544\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.092098\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.091503\n",
            "\n",
            "Test set: Average loss: 0.0005, Accuracy: 9893/10000 (99%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.079300\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.044412\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.047062\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.059358\n",
            "\n",
            "Test set: Average loss: 0.0005, Accuracy: 9891/10000 (99%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.066415\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.085709\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.049540\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.055823\n",
            "\n",
            "Test set: Average loss: 0.0005, Accuracy: 9891/10000 (99%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.053437\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.090781\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.069287\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.055730\n",
            "\n",
            "Test set: Average loss: 0.0005, Accuracy: 9886/10000 (99%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.065915\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.054461\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.084972\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.068745\n",
            "\n",
            "Test set: Average loss: 0.0005, Accuracy: 9891/10000 (99%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.039954\n",
            "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.051358\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.066614\n",
            "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.060896\n",
            "\n",
            "Test set: Average loss: 0.0005, Accuracy: 9891/10000 (99%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.126906\n",
            "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.072182\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.087911\n",
            "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.062964\n",
            "\n",
            "Test set: Average loss: 0.0005, Accuracy: 9890/10000 (99%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.044854\n",
            "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.085508\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.136378\n",
            "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.054732\n",
            "\n",
            "Test set: Average loss: 0.0005, Accuracy: 9886/10000 (99%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.069868\n",
            "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.052300\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.077918\n",
            "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.079981\n",
            "\n",
            "Test set: Average loss: 0.0005, Accuracy: 9888/10000 (99%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hint --> 98% Model (Fine-tuning)"
      ],
      "metadata": {
        "id": "lJc5BYvYGppk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params_to_update = []\n",
        "for name, param in model.named_parameters():\n",
        "  if 'conv1' in name:\n",
        "    print(name)    \n",
        "    param.requires_grad = True\n",
        "    params_to_update.append(param)\n",
        "  elif 'fc' in name:\n",
        "    print(name)    \n",
        "    param.requires_grad = True\n",
        "    params_to_update.append(param)\n",
        "  else:\n",
        "    param.requires_grad = False\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "JNI3rm7VGEFr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bfabaa9-5913-4e3c-8f20-c70dec454f63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight\n",
            "layer1.0.conv1.weight\n",
            "layer1.1.conv1.weight\n",
            "layer2.0.conv1.weight\n",
            "layer2.1.conv1.weight\n",
            "layer3.0.conv1.weight\n",
            "layer3.1.conv1.weight\n",
            "layer4.0.conv1.weight\n",
            "layer4.1.conv1.weight\n",
            "fc.weight\n",
            "fc.bias\n",
            "ResNet(\n",
            "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    }
  ]
}